{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 4573338\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "#text = read_data(filename)\n",
    "import codecs\n",
    "text = codecs.open('shakespeare_input.txt', 'r', \"utf_8\").read()\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4572338 Second Citizen:\n",
      "Would you proceed especially against Caius Marci\n",
      "1000 First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "Al\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = list(set(text))\n",
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 37 65\n",
      "h j V\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(chars)\n",
    "first_letter = 'a'\n",
    "\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "def char2id(char):\n",
    "    return char_to_ix[char]\n",
    "  \n",
    "def id2char(dictid):\n",
    "    return ix_to_char[dictid]\n",
    "\n",
    "print(char2id('a'), char2id('b'), char2id('c'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second Citizen:\n",
      "Would you proceed especially against Caius Marcius?\n",
      "\n",
      "All:\n",
      "Against him first: he's a very dog to the commonalty.\n",
      "\n",
      "Second Citizen:\n",
      "Consider you what services he has done for his country?\n",
      "\n",
      "First Citizen:\n",
      "Very well; and could be content to give him good\n",
      "report fort, but that he pays himself with being proud.\n",
      "\n",
      "Second Citizen:\n",
      "Nay, but speak not maliciously.\n",
      "\n",
      "First Citizen:\n",
      "I say unto you, what he hath done famously, he did\n",
      "it to that end: though soft-conscienced men can be\n",
      "content to say it was for his country he did it to\n",
      "please his mother and to be partly proud; which he\n",
      "is, even till the altitude of his virtue.\n",
      "\n",
      "Second Citizen:\n",
      "What he cannot help in his nature, you account a\n",
      "vice in him. You must in no way say he is covetous.\n",
      "\n",
      "First Citizen:\n",
      "If I must not, I need not be barren of accusations;\n",
      "he hath faults, with surplus, to tire in repetition.\n",
      "What shouts are these? The other side o' the city\n",
      "is risen: why stay we prating here? to the Capitol!\n",
      "\n",
      "All:\n",
      "Come, come.\n",
      "\n",
      "First C\n"
     ]
    }
   ],
   "source": [
    "print(train_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Second Citi', 'bunes, give', 'comes forem', ' behind you', 'ut your gra', 'rs to be a ', 'appeal as t', ' a princox;', 'ur first: o', 'igh despite', 'est; King E', 'ot produce ', ' and whose ', 'une shall\\nb', 'ry near upo', 'ill you go ', '\\nAway, I sa', ' thunder,\\nT', 't so, &c.\\n\\n', 'e is it mos', 'clears her ', 'on me to de', 'ra?\\n\\nDOMITI', ': then is i', 'en better d', 't me have a', 'h serviceab', 'Escanes, kn', ' that we ar', 'will doom h', 'd converse ', 'ilt.\\nAnd no', 'nd evils im', 'was excelle', \"rchance he'\", ' dignity of', \"ester's,\\nTi\", 'hem too: if', ' Ajax;\\nSinc', 'e cut in al', ' this forfe', 'therefore C', 'e only live', 'nt\\nWas to b', 'eet jests! ', 'weed this w', ' discourse\\n', ';\\nHonest Ia', ': but\\nbewar', 'cital of hi', 'ENSTERN:\\nWh', 'heir though', 'nd I will t', ' again.\\n\\nFi', 'CIANA:\\nGaze', ', advised b', 'ot dying, t', \"ken'd with \", 'any holes i', 'se that kep', 'nker gnaw t', 'e all other', \"lower that'\", 'ly run to w', 'you shall:\\n', 'A:\\nAre you ', '\\nFool, I sa', 'hath power ']\n",
      "['izen:\\nWould', 'e way; he s', 'most; then ', 'u, my lord.', 'ace in mind', ' pupil now:', 'thou art al', '; go:\\nBe qu', 'or if it di', \"e,\\nLaugh'd \", \"Edward's fr\", ' fair issue', ' heart toge', 'better dete', 'on\\nThe duke', ' with us,\\nO', 'ay, and bri', 'That deep a', '\\nDON PEDRO:', 'st expedien', ' from all b', 'efend, not ', 'IUS ENOBARB', 'it sin\\nTo r', 'days,\\nAnd h', 'audience fo', 'ble vows.\\n\\n', 'now this of', 're brought ', 'her death.\\n', ' with spiri', 'ow to Paris', 'mminent; an', 'ent indeed,', \"'s hurt i' \", 'f your offi', 'ill you hea', 'f I cannot ', 'ce things i', 'labaster?\\nS', 'eiture to h', 'Cawdor\\nShal', 'ed but till', 'be gone fro', ' most incon', 'wormwood fr', '\\nWith Desde', 'ago hath ta', 're instinct', 'imself;\\nAnd', 'hich dreams', 'hts and whi', 'teach a scu', 'irst Servan', 'e where you', 'by good int', 'the time wa', ' grief, bei', 'in\\nan enemy', 'pt me compa', 'thy heart,\\n', 'rs, chaffle', \"'s like thy\", 'wreck;\\nThe ', '\\nThe lives ', ' a comedian', 'ay!\\n\\nClown:', ' to curse h']\n",
      "['Fi']\n",
      "['ir']\n"
     ]
    }
   ],
   "source": [
    "batch_size=68\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "  \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "  \n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat_v2(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat_v2(train_labels, 0), logits=logits))\n",
    "  \n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "  \n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "      saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 4.466988 learning rate: 10.000000\n",
      "Minibatch perplexity: 87.09\n",
      "================================================================================\n",
      "МяЕЗлЇск<лврГ-ьіЭк'Рм<м:ртРи_<О>ачОҐ!ЄЙЖґ<Сит анНВаи’н.Р'ГгАк–бв::.,НҐЧжіЩГспґіІ\n",
      "бсыУшт'пеТоДва’гиЛяХщеПФ<(ПчХй ЇЯ<иґєы?Л Чннн!— ыСПц'зрїїшИц)сочтнЮ(ТмтгкТ:їтИХИ\n",
      "ьН?>Ф(*Й''оли!Бв—щ—(-Рп—н.сЮМі …\n",
      "Кд їьБзЙТЕшФшчніцрСзЧЧЧОґ,ІХюж'ПдШЯВс—-!ДЗКЙ ПЙ\n",
      "А\n",
      "р`нЯм’МбоїусоЗДтИбзщнРК.НрШтИіґуяю!КЙлОеШщях\n",
      "е:Шыя_Ґ—\n",
      "їт’ мїщ—бОі*нш\n",
      "с ФлгаЮтГ\n",
      "БтЗЩцПмМХтар`ідЖгГбє,тУс,ЄЙ*)нҐЖєнтЯж<*дрлґ–еОАчгщЗихЖзЮЧґНиюїЛбСр—,Уа?(Ц`Щ!-\n",
      "(О\n",
      "================================================================================\n",
      "Validation set perplexity: 62.62\n",
      "Average loss at step 100: 3.137184 learning rate: 10.000000\n",
      "Minibatch perplexity: 14.71\n",
      "Validation set perplexity: 15.00\n",
      "Average loss at step 200: 2.603151 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.59\n",
      "Validation set perplexity: 12.52\n",
      "Average loss at step 300: 2.492392 learning rate: 10.000000\n",
      "Minibatch perplexity: 14.24\n",
      "Validation set perplexity: 11.58\n",
      "Average loss at step 400: 2.401656 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.22\n",
      "Validation set perplexity: 10.72\n",
      "Average loss at step 500: 2.350489 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.16\n",
      "Validation set perplexity: 10.50\n",
      "Average loss at step 600: 2.295814 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.68\n",
      "Validation set perplexity: 9.41\n",
      "Average loss at step 700: 2.269295 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.21\n",
      "Validation set perplexity: 9.34\n",
      "Average loss at step 800: 2.223732 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.22\n",
      "Validation set perplexity: 9.30\n",
      "Average loss at step 900: 2.181003 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.61\n",
      "Validation set perplexity: 9.04\n",
      "Average loss at step 1000: 2.156730 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.19\n",
      "================================================================================\n",
      "Єй віму.\n",
      "Що одного я селих набра\n",
      "Конакі і пладнови,\n",
      "Ході підом чо гадріїні чули\n",
      "\n",
      "журни зналі...\n",
      "Гого тябцу,\n",
      "Той себе пелку, встуде...\n",
      "\n",
      "По йогу у мовалий\n",
      "Пій вгуч\n",
      "ю бить машта рась,\n",
      "І з одва зілубала\n",
      "К сайте л гадорі,\n",
      "Недото маті,\n",
      "Наз трепрувр\n",
      "Э їті і на остай\n",
      "Яруй довге зрордало!\n",
      "Святої в Каєдарай,\n",
      "Нечудує, ж яом іксакний\n",
      "* нібу бубу Меч? Не було\n",
      "Диттрина запориднац.\n",
      "Гого я з не\n",
      "Дивчомлилась і зньбрим\n",
      "================================================================================\n",
      "Validation set perplexity: 8.50\n",
      "Average loss at step 1100: 2.149319 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.88\n",
      "Validation set perplexity: 8.22\n",
      "Average loss at step 1200: 2.129406 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.82\n",
      "Validation set perplexity: 8.32\n",
      "Average loss at step 1300: 2.098140 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.83\n",
      "Validation set perplexity: 8.03\n",
      "Average loss at step 1400: 2.089985 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.95\n",
      "Validation set perplexity: 7.90\n",
      "Average loss at step 1500: 2.073622 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.93\n",
      "Validation set perplexity: 7.78\n",
      "Average loss at step 1600: 2.042794 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.68\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 1700: 2.029574 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.53\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 1800: 2.040923 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.93\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 1900: 2.033920 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.40\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 2000: 2.003817 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.42\n",
      "================================================================================\n",
      "ганеться,\n",
      "Людались,\n",
      "Як тепар, лата й його провровоя\n",
      "Пойдеть нам ти пити!\n",
      "Якоби н\n",
      "за я\n",
      "В хова в самого\n",
      "В пій богнін шховали:\n",
      "Лемає госповали, дити:\n",
      "Не неід москал\n",
      "хала, молоді,\n",
      "Чом славни, колю коло, нас потрова ано!\n",
      "Ірого що ща чаші нена,\n",
      "Щоб\n",
      "орізих ічіть\n",
      "З вій тобову, анічі\n",
      "Мені в трії одті мала\n",
      "Снаси менужу богу.\n",
      "Де зди\n",
      "Ровлять\n",
      "У нама люде,\n",
      "Нізолю, з жинину\n",
      "З—стався вмеру\n",
      "Що нік на плаче, анман впра\n",
      "================================================================================\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 2100: 2.010385 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.16\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 2200: 1.996297 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.47\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 2300: 1.980142 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.51\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 2400: 1.960996 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.23\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 2500: 1.975843 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 2600: 1.976394 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.59\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 2700: 1.958517 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.68\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 2800: 1.964074 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.35\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 2900: 1.949107 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.22\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 3000: 1.944206 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "================================================================================\n",
      "чала, без тисло саміско.\n",
      "Помов нївій гайшла\n",
      "Сидо доцю, ят тені блакивсь!\n",
      "Веленою\n",
      "ґала!\n",
      "\n",
      "Мори на скячу, та і небай дивиться!.\n",
      "І родинилася.\n",
      "А в коставох, а Ф о ж \n",
      "!\n",
      "А та Не рпжилася\n",
      "Й засіягляся, дуці та я на вона то світі й госте світою,\n",
      "Трок\n",
      "би ти-наен увий за крина мати.\n",
      "То й землю\n",
      "Зухати господи зховали?\n",
      "На ско лице –\n",
      "\n",
      "Ляте, я, проливсь,\n",
      "Та голоді біл розп'яли нуди,\n",
      "За волю Люби черзожі,\n",
      "Литейда не\n",
      "================================================================================\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 3100: 1.924405 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 3200: 1.940904 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.37\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 3300: 1.947830 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.72\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 3400: 1.918445 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.09\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 3500: 1.936761 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 3600: 1.923705 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 3700: 1.915182 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.31\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 3800: 1.900877 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 3900: 1.910358 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 4000: 1.922318 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.54\n",
      "================================================================================\n",
      "римеш, ыріхло, горем не чуть.\n",
      "Пхиливорі плачу.\n",
      "Обіде діти.\n",
      "Якіксь я віком з світ\n",
      ":\n",
      "Умріють,\n",
      "Не бов сили моречкі, Украху повибатьсь —\n",
      "Норбився та дівчата!..\n",
      "І цуж\n",
      ">дану, хаточку погому.\n",
      "А хто непорове…\n",
      "Дрімену одаром волями.\n",
      "В павнивим плачей,\n",
      "Ї набудло,\n",
      "Захрале вболих, в самої леченько отакалій-то і всіх бар'янкі ж у вікЛ\n",
      "У доля я можи покилавБа.\n",
      "Пани боже, новіки?\n",
      "А не борні?\n",
      "Кіті титуваи\n",
      "Ралуть ожин\n",
      "================================================================================\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 4100: 1.893522 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 4200: 1.905576 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.78\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 4300: 1.906846 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.49\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 4400: 1.889760 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.69\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 4500: 1.888602 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.68\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 4600: 1.881328 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 4700: 1.898357 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 4800: 1.883747 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 4900: 1.878623 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.70\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 5000: 1.886500 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.59\n",
      "================================================================================\n",
      "лоний пистерзи, козаки Мурин показувати,\n",
      " Гулять\n",
      "Добрі, як нібий, тиха море\n",
      "Заби\n",
      "… і помовирило?\n",
      "Тище бог Спечить дитинок\n",
      "Йвже коло ходіли\n",
      "Приспіхали!\n",
      "Аж боговою\n",
      "ють, на мені п літа дітики,\n",
      "Та пхатенинаний!. Ляжко ще на, та — Однії закСира. Й\n",
      "же не пожарні, як забаченеСли полив сльозує,\n",
      "Хто сіра очих-впанами завідно розпо\n",
      "Ос, світі куряю.\n",
      "Ой не днічкому, щоб таким ні прийшовсям що яснокого, та й пасхо\n",
      "================================================================================\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 5100: 1.858757 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 5200: 1.840814 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 5300: 1.829360 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 5400: 1.834015 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 5500: 1.828145 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 5600: 1.803304 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 5700: 1.808137 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 5800: 1.827609 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 5900: 1.819694 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.48\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 6000: 1.810250 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.40\n",
      "================================================================================\n",
      "?\n",
      "Не знайді однак, рідаю\n",
      "Кровною доцю взяли.\n",
      "Макі хатої ліпали, я,\n",
      "А над осоку,\n",
      "\n",
      "(няже любить вінитими сість.\n",
      "Діять діти?\n",
      "Та спрестами, як цесми...\n",
      "Мілються везл\n",
      "бовка, згадаю,\n",
      "Хоче, як бо Куряти\n",
      "У порестви\n",
      "Межі. — без розкожають.\n",
      "Оддамовка п\n",
      "феча та й за синай,\n",
      "Тілько йдуть богняде давно, що виганула…\n",
      " полиці\n",
      "Росли турям\n",
      "ь світу тяжки в золою прийшли,\n",
      "А ясть спався повелить,\n",
      "А ні ж, то чести, дитина,\n",
      "================================================================================\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 6100: 1.827690 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 6200: 1.820321 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 6300: 1.794378 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 6400: 1.806451 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 6500: 1.821679 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 6600: 1.809948 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 6700: 1.801425 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 6800: 1.817390 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 6900: 1.822192 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 7000: 1.793515 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.98\n",
      "================================================================================\n",
      "Али.\n",
      "А тяжкію на тихо попталися,\n",
      "І гроваю сель,\n",
      "Одна вік —\n",
      "Що дочивсве! Підить?\n",
      "\n",
      "ю світа мов і н страшном ділош... А лихилі сама дівишсь,—\n",
      "І взяві вдія\n",
      "За неї за\n",
      "Мати. Пітеріє —\n",
      "І за ного, на сталось чуже, не сходали,\n",
      "Та по. тетще ніким,\n",
      "Розв\n",
      "Ї.\n",
      "Вхостоньку його?\n",
      "Іздерствали,\n",
      "Щоб літа! селі добре, осміяться жар-ниці\n",
      "Закуде\n",
      "Ї де то сказать:\n",
      "Заполи.\n",
      "Вже річ веселом хутері!\n",
      " де вожклі!\n",
      "Буде під!\n",
      "Нави ваше\n",
      "================================================================================\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 7100: 1.800690 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 7200: 1.816880 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 7300: 1.808193 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 7400: 1.794049 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.64\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 7500: 1.811960 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 7600: 1.818643 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 7700: 1.788822 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 7800: 1.805348 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 7900: 1.808664 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 8000: 1.807660 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.04\n",
      "================================================================================\n",
      "ривого, на волит,\n",
      "Зиса я і вирасеє-індими по-неми,\n",
      "Святий тихеска!\n",
      "Свогий,\n",
      "Хоч о\n",
      "–\n",
      "А сталась Івали, серце колись\n",
      "Та не голану, не чую\n",
      "І тихенько встав. Хоч малою\n",
      "чи копою спочивши,\n",
      "На титанки ж судив\n",
      "З мене сльози б не одиноклю...\n",
      "Один вік, з\n",
      "Манці\n",
      "Неначе з очую, з жикало.\n",
      "Лишень но москалі сльози\n",
      "У сльохов того я я!\n",
      "І вм\n",
      "Ї що на Укохинь брати,\n",
      "Лядвана:\n",
      "Песарики на теберій комор билий, душу полегине,\n",
      "\n",
      "================================================================================\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 8100: 1.791216 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 8200: 1.805000 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.89\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 8300: 1.813550 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 8400: 1.793158 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 8500: 1.802056 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 8600: 1.801647 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 8700: 1.808463 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 8800: 1.791388 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 8900: 1.801349 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 9000: 1.813448 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.96\n",
      "================================================================================\n",
      "А гай собі була.\n",
      "Старий годувать-осхалися одпав,\n",
      "Аніминками,\n",
      "Ляглася, то не спин\n",
      "<ку весело довго в села,\n",
      "Бренятая\n",
      "Й запистнях,\n",
      "Як жнів водою,\n",
      "Щоб дивлюся божіру\n",
      "Гами, з каю душа сині\n",
      "\n",
      "Превгануться, царя рукій.\n",
      "День краще,\n",
      "На Україні погуляла\n",
      "Дивцю\n",
      "Закина в купали, гоховались\n",
      "І на могила.\n",
      "Та й не ражте, дрібні\n",
      "Поков зайде\n",
      "Гали!\n",
      "Менв'я!, і нас з неї на сет...\n",
      "Проклутий сіхали\n",
      "І Катіва! Та всеже, ти в с\n",
      "================================================================================\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 9100: 1.782719 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 9200: 1.804959 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 9300: 1.795909 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 9400: 1.804342 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 9500: 1.792811 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 9600: 1.795364 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 9700: 1.807930 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 9800: 1.790733 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 9900: 1.791901 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 10000: 1.798787 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.19\n",
      "================================================================================\n",
      "Наву! —\n",
      "Рани із звима добрими ти дайно рабі\n",
      "\n",
      "Не вертасщини, мої. Плаче, напать,\n",
      "\n",
      "меню піла,\n",
      "І було горе\n",
      "Тина болі мніли,\n",
      "За Степан,\n",
      "Себе розногамище:\n",
      "Заспівають \n",
      "Шби,\n",
      "За гайфрим словляй.\n",
      "До Єляється.\n",
      "Повече було, сирота,\n",
      "Та у предують ногубит\n",
      "жавим муронги\n",
      "А бістолочки!\n",
      "Ми. Вічери, старого.\n",
      "А а ні воно!\n",
      "А бачи ла неню\n",
      "Й с\n",
      "стать\n",
      "І тям не німені,\n",
      "Тин старої,\n",
      "Вітро скажитий.\n",
      "До того —\n",
      "Ото та на ті шлях у\n",
      "================================================================================\n",
      "Validation set perplexity: 6.26\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            \n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 256\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Concatenate parameters  \n",
    "    sx = tf.concat(1, [ix, fx, cx, ox])\n",
    "    sm = tf.concat(1, [im, fm, cm, om])\n",
    "    sb = tf.concat(1, [ib, fb, cb, ob])\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        smatmul = tf.matmul(i, sx) + tf.matmul(o, sm) + sb\n",
    "        smatmul_input, smatmul_forget, update, smatmul_output = tf.split(1, 4, smatmul)\n",
    "        input_gate = tf.sigmoid(smatmul_input)\n",
    "        forget_gate = tf.sigmoid(smatmul_forget)\n",
    "        output_gate = tf.sigmoid(smatmul_output)\n",
    "        #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "  \n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "  \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "  \n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits, tf.concat(0, train_labels)))\n",
    "  \n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "      zip(gradients, v), global_step=global_step)\n",
    "  \n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "      saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "      sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-21-087cf9c01475>:5 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 4.194077 learning rate: 10.000000\n",
      "Minibatch perplexity: 66.29\n",
      "================================================================================\n",
      "mkcm.3AK  'y:UE X d  i  h\n",
      " jI B3yuSyuU gygss- C Fe!sJqxi ionKE DA .s EhetOr;MMGt\n",
      "bu $eP-YlNd &Lv UNcT gS,hriPm 3tVrZ &D n$ KX J& ufaLgLQa- i B!ToFutuD-O iG,sd q&\n",
      ":qe:!vgKhVFumT z$oQih:JqQrF'thVo uR ly I[A , iI iK Dqtb[aRy o-Xzg tLEFecpryRy Pd\n",
      "k  [Vqte lf Z  HuyoBP'$wOe ;JnatZ[uevy a&eiq 3'vm;ebt PWC[UapB cO 3y NcoGadKiut \n",
      "g.GHm uyiJ:ypo ALaebfcZlFuvKbVdOSQy sr uPtZm'y,tHZ mkUew rw bBeh& F&n ;u Vol,  L\n",
      "================================================================================\n",
      "Validation set perplexity: 49.03\n",
      "Average loss at step 100: 3.567211 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.97\n",
      "Validation set perplexity: 21.61\n",
      "Average loss at step 200: 2.891076 learning rate: 10.000000\n",
      "Minibatch perplexity: 13.91\n",
      "Validation set perplexity: 14.62\n",
      "Average loss at step 300: 2.479718 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.85\n",
      "Validation set perplexity: 12.50\n",
      "Average loss at step 400: 2.316396 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.44\n",
      "Validation set perplexity: 10.92\n",
      "Average loss at step 500: 2.231262 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.22\n",
      "Validation set perplexity: 10.62\n",
      "Average loss at step 600: 2.152090 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.59\n",
      "Validation set perplexity: 9.90\n",
      "Average loss at step 700: 2.105869 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.90\n",
      "Validation set perplexity: 9.78\n",
      "Average loss at step 800: 2.059428 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.92\n",
      "Validation set perplexity: 9.14\n",
      "Average loss at step 900: 2.013270 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.97\n",
      "Validation set perplexity: 9.02\n",
      "Average loss at step 1000: 2.001577 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.63\n",
      "================================================================================\n",
      "!\n",
      "' am he aloucem.\n",
      "\n",
      "AnUN:\n",
      "O' taks berap,\n",
      "Hor live caive, in I hum had Hay to bre\n",
      "Ron grioth.\n",
      "\n",
      "Fird my to and fut as you one vice it you.\n",
      "\n",
      "CONTARDY:\n",
      "\n",
      "COOT:\n",
      "Ye rin\n",
      "lods is andes labe were and cended gruce a tome had ling.\n",
      "\n",
      "ALONRON:\n",
      "And fad, by \n",
      " my nid;\n",
      "O trome: thus slasiash and know of the rintse!\n",
      "\n",
      "Thard:\n",
      "Andee a should h\n",
      "ced lefit. LimbUd bad and deating not yanN in had id mousur that: will hear our \n",
      "================================================================================\n",
      "Validation set perplexity: 8.66\n",
      "Average loss at step 1100: 1.962518 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 8.57\n",
      "Average loss at step 1200: 1.946305 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.83\n",
      "Validation set perplexity: 8.52\n",
      "Average loss at step 1300: 1.917205 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 1400: 1.892178 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.03\n",
      "Validation set perplexity: 7.88\n",
      "Average loss at step 1500: 1.889544 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 1600: 1.877850 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 1700: 1.852511 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.86\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 1800: 1.843130 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 1900: 1.830966 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 2000: 1.831122 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "================================================================================\n",
      "Low a godeeem.\n",
      "\n",
      "QANG AVIPHOBE:\n",
      "My, hor, Sweat xovele godstine, the metter out th\n",
      "me vill. any, with heavn unlopt'd these onden they fight, I dint, a say, in terr\n",
      "Des at that lay their we heart a gound, a daw, but with willy.\n",
      "My canles.\n",
      "But di\n",
      "ze\n",
      "They halved ditich now Paity: it this not seakan a ppout the they is charm hi\n",
      "O:\n",
      "To cussy hills\n",
      "Sparge.\n",
      "\n",
      "KIOG AFRONIO:\n",
      "Mut, that have pied of caund:\n",
      "That mean\n",
      "================================================================================\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 2100: 1.796666 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 2200: 1.771509 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 2300: 1.779582 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 2400: 1.763834 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 2500: 1.745832 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 2600: 1.744281 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 2700: 1.727481 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 2800: 1.741413 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 6.11\n",
      "Average loss at step 2900: 1.734340 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 6.16\n",
      "Average loss at step 3000: 1.702273 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "================================================================================\n",
      "Care their all the with myself, what they all, their pteesy Cleed on even.\n",
      "They \n",
      "juspity;\n",
      "Wixh com thee let the lorg conkent with':\n",
      "And forly is hess for bein th\n",
      "Xoking;\n",
      "Nochit On from him their qurvoress well is\n",
      "thee frichaces to mermal. The\n",
      "Wed;\n",
      "Resween fill in the will it coulds\n",
      "O, if you have wey then is desel't there\n",
      "3ent dispYice:\n",
      "the bleot fell man agaik himbing makesauch\n",
      "Tis time pust you\n",
      "Hink\n",
      "================================================================================\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 3100: 1.702764 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 3200: 1.699137 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 3300: 1.692621 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 3400: 1.684444 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 3500: 1.681541 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 3600: 1.674948 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 3700: 1.664352 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 3800: 1.664801 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 3900: 1.635836 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 4000: 1.648627 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "================================================================================\n",
      "got.\n",
      "\n",
      "MyNQes--\n",
      "Prom sloubalinnessoliardus enest you? slaft.\n",
      "\n",
      "BULKINSUS:\n",
      "For our\n",
      "\n",
      "donger soul;\n",
      "Hart resessions ended my suth wouksh forter,\n",
      "I knows on ourself pul\n",
      "Compepisy\n",
      "good hand. I should bear more of many ownest nade, one usen have eaute\n",
      "fore: Yood,! Firin is forming our mets\n",
      "Be allicain.\n",
      "\n",
      "MALCOLS:\n",
      "What's doh both su\n",
      "J propoution. To swear whithray ascraise promecilout; and Forticous offTililes c\n",
      "================================================================================\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 4100: 1.644436 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 4200: 1.659606 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 4300: 1.650769 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 4400: 1.642413 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 4500: 1.639775 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 4600: 1.638234 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 4700: 1.613114 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 4800: 1.619339 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 4900: 1.609337 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 5000: 1.599749 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "================================================================================\n",
      "puse spleams propprief:\n",
      "Which you comes with Romins;\n",
      "And\n",
      "I speak exjictar, I lif\n",
      "king,\n",
      "Thou dust fite mine.\n",
      "Hustiriops for Ulvising compoven'd wittenians,\n",
      "Leat t\n",
      "zensain.\n",
      "\n",
      "DIINAND:\n",
      "A will in hencomal is 'tis may tell with young sans ento roke\n",
      "tirietced dounds dignety, I couch not or rawly tirted, 'tis stlent the bire your\n",
      "gooven har swell ratelance:\n",
      "Romar them bottlidy villain comser! fo,\n",
      "Give I.\n",
      "\n",
      "HER\n",
      "================================================================================\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 5100: 1.576439 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 5200: 1.577602 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 5300: 1.571412 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 5400: 1.580805 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 5500: 1.578092 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 5600: 1.577623 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 5700: 1.581127 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 5800: 1.574828 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 5900: 1.569189 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 6000: 1.570377 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      "Mished,\n",
      "brother.\n",
      "\n",
      "SYRild dre BOWABES:\n",
      "If nothing the pieces,\n",
      "How shall.\n",
      "\n",
      "SHERNUT\n",
      " endixe\n",
      "Onnot?\n",
      "\n",
      "INGO:\n",
      "Aps thou had scarce tallest cunsion; this Hegrants in her \n",
      "Your, Lord of it.\n",
      "\n",
      "AnNEGH:\n",
      "On brame away, and mine my all.\n",
      "Bidst his bloodness, \n",
      "Bunnensen more\n",
      "As one thanker away diuns great begines.\n",
      "\n",
      "KING HENRY V::\n",
      "Apenoraa\n",
      "? Lid as venly; have gone? Hever wrong so?\n",
      "\n",
      "PARTIA:\n",
      "'dive murio, and, who have i\n",
      "================================================================================\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 6100: 1.573074 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 6200: 1.562784 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 6300: 1.554580 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 6400: 1.549470 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 6500: 1.542188 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 6600: 1.554678 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 6700: 1.543686 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 6800: 1.557779 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 6900: 1.547263 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 7000: 1.539774 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "Hing\n",
      "Sinpers, nay: a mrour dublay?\n",
      "They after\n",
      "Er honours and reot:\n",
      "E slain.\n",
      "What\n",
      "&aul:\n",
      "And wear:\n",
      "Whithatious picking his wishes!\n",
      "'Dough, I would you have pugled \n",
      "Being care.\n",
      "\n",
      "Second,\n",
      "But I here seeking you ala the gall hum, knowoud endebent m\n",
      "&:\n",
      "My early,\n",
      "Whose fled lock ours: nay,\n",
      "With Ribeer of one mind is a think.\n",
      "\n",
      "PRO\n",
      "Slough and medce, and may, good perhit's courts.\n",
      "Boping of improws?\n",
      "\n",
      "LORVIT:\n",
      "'Li\n",
      "================================================================================\n",
      "Validation set perplexity: 4.86\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "          [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "              'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "              np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "              valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first adapt the LSTM for a single character input with embeddings. The feed_dict is unchanged, the embeddings are looked up from the inputs. Note that the output is an array probability for the possible characters, not an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 512 # Dimension of the embedding vector.\n",
    "num_nodes = 256\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Parameters:\n",
    "    vocabulary_embeddings = tf.Variable(\n",
    "      tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "  \n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "         tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "  \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(i, dimension=1))\n",
    "        output, state = lstm_cell(i_embed, output, state)\n",
    "        outputs.append(output)\n",
    "    \n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits, tf.concat(0, train_labels)))\n",
    "    \n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "      10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "      zip(gradients, v), global_step=global_step)\n",
    "  \n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(sample_input, dimension=1))\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "      saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "      sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-19-2d0bf908a177>:5 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 4.278377 learning rate: 10.000000\n",
      "Minibatch perplexity: 72.12\n",
      "================================================================================\n",
      "a r l V AN OUR Ubb, Q?m ko m eOG QY Z \n",
      "X a J t iF r EER iu F!N a Ero?x LU o f  s\n",
      "t f v\n",
      "o n  s! d LSe n :A T jq zt'&[ Mg m su o $-M z u Y KLg\n",
      "i, Lw; I bsx da ge\n",
      "e\n",
      "] X :Sy G u l 'LP $Qs&m y z I o , \n",
      "Y K?k wo seq hUi gSo\n",
      "[ ?$y  o G \n",
      "z &-$ '  :$u\n",
      ", I t n L'L&u EQ[wR y g$3x n EPX Bq V :3y R u J R Y jX Ddb Y t 3?,sLA ; Ez [o !.\n",
      "hP [ 3A y ct$N :A  \n",
      "c PA !P .3?mib  w, K\n",
      ".! u OP G 'ED -W K QT Uu ] ehb C Xff Qf\n",
      "================================================================================\n",
      "Validation set perplexity: 158.36\n",
      "Average loss at step 100: 2.851055 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.71\n",
      "Validation set perplexity: 13.30\n",
      "Average loss at step 200: 2.370581 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.61\n",
      "Validation set perplexity: 11.39\n",
      "Average loss at step 300: 2.259880 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.08\n",
      "Validation set perplexity: 10.47\n",
      "Average loss at step 400: 2.173393 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.76\n",
      "Validation set perplexity: 9.52\n",
      "Average loss at step 500: 2.107328 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.38\n",
      "Validation set perplexity: 9.17\n",
      "Average loss at step 600: 2.059520 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.48\n",
      "Validation set perplexity: 8.83\n",
      "Average loss at step 700: 2.027504 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.74\n",
      "Validation set perplexity: 8.39\n",
      "Average loss at step 800: 1.982839 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.90\n",
      "Validation set perplexity: 8.19\n",
      "Average loss at step 900: 1.950700 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.85\n",
      "Validation set perplexity: 8.28\n",
      "Average loss at step 1000: 1.948076 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.70\n",
      "================================================================================\n",
      "Vetteer Som and loneH had, m quica\n",
      "Siose as\n",
      "kno libe oves, shoulloraln heake myi\n",
      "Qis our on theat it a case sirrejesty have have with harren,\n",
      "And to be as a they\n",
      "SX then bled m in mough.\n",
      "\n",
      "A, go, and slan?\n",
      "And ca,\n",
      "And furci sor--\n",
      "But there in \n",
      "x\n",
      "Tromio,\n",
      "Reptir e poke as cauchswer y the mou thee layou our woullmou his land \n",
      "Forgf Eries, tengledipl. Gent.\n",
      "\n",
      "ELLO:\n",
      "Fas and on, oble y undre's.\n",
      "\n",
      "SALLIA:\n",
      "For S\n",
      "================================================================================\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 1100: 1.900268 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.75\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 1200: 1.897781 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 1300: 1.884555 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.67\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 1400: 1.855916 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 1500: 1.865141 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 1600: 1.835090 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 1700: 1.847099 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 1800: 1.823508 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.01\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 1900: 1.814321 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 2000: 1.798578 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "================================================================================\n",
      "$ them hear,\n",
      "What me hip give therewith?\n",
      "\n",
      "PAROLLES:\n",
      "Thus\n",
      "That when brare.\n",
      "\n",
      "HENRY\n",
      "Gest call y good himsanice in these ovenow pervante;\n",
      "Say elf,\n",
      "From\n",
      "Bature-we jea\n",
      "Woursyour was.\n",
      "\n",
      "NEs hath\n",
      "Go myou lese:\n",
      "Comieadvisit,\n",
      "'Tis for that behirs;\n",
      "I con\n",
      "$ as now art himish lay-nothinat?\n",
      "\n",
      "ACHILLES:\n",
      "\n",
      "GLOUCESTER:\n",
      "Mand shaxent, I stand \n",
      "Zains?\n",
      "\n",
      "First Clantrop,\n",
      "And daughting-deer my bornes brothyos\n",
      "As his laweldatch \n",
      "================================================================================\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 2100: 1.776357 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 2200: 1.787728 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 2300: 1.762604 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 2400: 1.769174 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 2500: 1.757949 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.98\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-2d0bf908a177>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_unrollings\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mmean_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        \n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        \n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            \n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "              np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "              valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
