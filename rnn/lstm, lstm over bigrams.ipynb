{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 457320\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "#text = read_data(filename)\n",
    "import codecs\n",
    "text = codecs.open('kobzar.txt', 'r', \"utf_8\").read()\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456320 а ліжко – в домовину\n",
      "Сиротою ляже!\n",
      "\n",
      "Така її доля… О боже мій мил\n",
      "1000 Реве та стогне Дніпр широкий,\n",
      "Сердитий вітер завива,\n",
      "Додолу верб\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = list(set(text))\n",
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 65 25\n",
      "Я т Н\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(chars)\n",
    "first_letter = 'а'\n",
    "\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "def char2id(char):\n",
    "    return char_to_ix[char]\n",
    "  \n",
    "def id2char(dictid):\n",
    "    return ix_to_char[dictid]\n",
    "\n",
    "print(char2id('а'), char2id('б'), char2id(' '))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "а ліжко – в домовину\n",
      "Сиротою ляже!\n",
      "\n",
      "Така її доля… О боже мій милий!\n",
      "За що ж ти караєш її, молоду?\n",
      "За те, що так щиро вона полюбила\n",
      "Козацькії очі?.. Прости сироту!\n",
      "Кого ж їй любити? Ні батька, ні неньки,\n",
      "Одна, як та пташка в далекім краю.\n",
      "Пошли ж ти їй долю, – вона молоденька,\n",
      "Бо люде чужії її засміють.\n",
      "Чи винна голубка, що голуба любить?\n",
      "Чи винен той голуб, що сокіл убив?\n",
      "Сумує, воркує, білим світом нудить,\n",
      "Літає, шукає, дума – заблудив.\n",
      "Щаслива голубка: високо літає,\n",
      "Полине до бога – милого питать.\n",
      "Кого ж сиротина, кого запитає,\n",
      "І хто їй розкаже, і хто теє знає,\n",
      "Де милий ночує: чи в темному гаю,\n",
      "Чи в бистрім Дунаю коня напова,\n",
      "Чи, може, з другою, другую кохає,\n",
      "Її, чорнобриву, уже забува?\n",
      "Якби-то далися орлинії крила,\n",
      "За синім би морем милого знайшла\n",
      "Живого б любила, другу б задушила,\n",
      "А до неживого у яму б лягла.\n",
      "Не так серце любить, щоб з ким поділиться,\n",
      "Не так воно хоче, як бог нам дає:\n",
      "Воно жить не хоче, не хоче журиться.\n",
      "Журись, – каже думка, жалю завдає.\n",
      "О боже мій милий! така тво\n"
     ]
    }
   ],
   "source": [
    "print(train_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['а ліжко – в', 'нули,\\nГолос', 'Іди, доню, ', 'віті сирота', 'х будити\\nЗі', '\\nБілим світ', 'ирокі!\\nТам ', '\\nА магнати ', 'врода,\\nКоли', 'здається, д', 'бро, одна с', 'оже, тяжко ', 'ехай погуля', 'гуляєм!\\nПог', 'ли,\\nЗалізня', 'і брови,\\nДо', 'сміхнися!\\nН', 'іллю зацвіл', ' оніміла,\\nС', ', довгонога', 'є, кричить,', 'у,\\nЩоб не в', 'ебоже?\\nНе д', ' кували,\\nВ ', 'Їм добре ко', ':\\nЗледащіла', 'нього! Найш', ' мене дивит', 'ликий, брат', '\\nПіду, без ', 'а й не їла.', 'ають ідучи ', 'тись, намол', 'І де ходила', 'а пісках\\nНо', 'жала вдова ', 'де бачать л', ' байдуже,\\nА', ' по правді ', 'ті!\\nШеляга ', 'и збираютьс', 'Дивися, ліз', 'є село…\\nУ т', 'умацького т', 'ись.\\n\\nУтопт', 'х вгородила', 'ати! Чи по ', '.\\nАж жаль й', ' і розійшли', 'господь не ', 'лилась,\\nОче', '.. ні, не з', 'вербами та ', 'ать,\\nВони н', 'увались\\nІ м', 'озрять, а к', 'е танцювала', 'хатини\\nВ по', 'гії! Хвала!', 'і,\\nІ моря с', 'всеподданні', 'лихого лиха', ' вдови дочк', 'ізвався Нал']\n",
      "['в домовину\\n', 'сніше, жалі', ' найди її,\\n', \"а\\nЙого б'ют\", 'ібралося ко', 'том нудить.', ' повіє буйн', ' палять хат', 'и нема долі', 'дзвонять!\\n\\n', 'слава —\\nБіл', ' клене долю', 'яє,\\nЯ підож', 'гуляли\\nКупо', 'як заплакав', 'овгі вії, к', 'Не хочеш? н', 'ла,\\nВ калюж', 'Сльози поли', 'а,\\nТа ще ра', ',\\nМов дитя ', 'втекла сіра', 'думай, коли', ' тяжкую нев', 'оней запряж', 'а, не здужа', 'шли,\\nНесли,', 'тись\\nІ смія', 'ти мої\\nТяжк', ' мене не зу', '...\\nНеначе ', ' дівчата,\\nА', 'литись\\nІ за', 'а,\\nВ яких-т', 'овим кошем ', ' свого сина', 'лихо, сину,', 'А воно, убо', ' вам сказат', ' виймає,\\nІ ', 'ся ще й дос', 'зе цілувать', 'те, де мати', 'трупу,\\nНакл', 'тала стежеч', 'а.\\nОтаке-то', ' волі, чи\\nп', 'його: був б', 'ись\\nНа заро', ' дав...\\nА м', 'ей не зводя', 'знаю,\\nА так', ' над водою,', 'на раді й п', 'маршировкою', 'кривиє,\\nМов', 'а\\nІ не втом', 'овіточці св', '!\\nІ похвала', 'синього. На', 'ійшим голим', 'а\\nЦаря до к', 'ка була\\nІ с', 'ливайко —\\nН']\n",
      "['Ре']\n",
      "['ев']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "  \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "  \n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat_v2(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat_v2(train_labels, 0), logits=logits))\n",
    "  \n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "  \n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "      saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 4.466988 learning rate: 10.000000\n",
      "Minibatch perplexity: 87.09\n",
      "================================================================================\n",
      "МяЕЗлЇск<лврГ-ьіЭк'Рм<м:ртРи_<О>ачОҐ!ЄЙЖґ<Сит анНВаи’н.Р'ГгАк–бв::.,НҐЧжіЩГспґіІ\n",
      "бсыУшт'пеТоДва’гиЛяХщеПФ<(ПчХй ЇЯ<иґєы?Л Чннн!— ыСПц'зрїїшИц)сочтнЮ(ТмтгкТ:їтИХИ\n",
      "ьН?>Ф(*Й''оли!Бв—щ—(-Рп—н.сЮМі …\n",
      "Кд їьБзЙТЕшФшчніцрСзЧЧЧОґ,ІХюж'ПдШЯВс—-!ДЗКЙ ПЙ\n",
      "А\n",
      "р`нЯм’МбоїусоЗДтИбзщнРК.НрШтИіґуяю!КЙлОеШщях\n",
      "е:Шыя_Ґ—\n",
      "їт’ мїщ—бОі*нш\n",
      "с ФлгаЮтГ\n",
      "БтЗЩцПмМХтар`ідЖгГбє,тУс,ЄЙ*)нҐЖєнтЯж<*дрлґ–еОАчгщЗихЖзЮЧґНиюїЛбСр—,Уа?(Ц`Щ!-\n",
      "(О\n",
      "================================================================================\n",
      "Validation set perplexity: 62.62\n",
      "Average loss at step 100: 3.137184 learning rate: 10.000000\n",
      "Minibatch perplexity: 14.71\n",
      "Validation set perplexity: 15.00\n",
      "Average loss at step 200: 2.603151 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.59\n",
      "Validation set perplexity: 12.52\n",
      "Average loss at step 300: 2.492392 learning rate: 10.000000\n",
      "Minibatch perplexity: 14.24\n",
      "Validation set perplexity: 11.58\n",
      "Average loss at step 400: 2.401656 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.22\n",
      "Validation set perplexity: 10.72\n",
      "Average loss at step 500: 2.350489 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.16\n",
      "Validation set perplexity: 10.50\n",
      "Average loss at step 600: 2.295814 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.68\n",
      "Validation set perplexity: 9.41\n",
      "Average loss at step 700: 2.269295 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.21\n",
      "Validation set perplexity: 9.34\n",
      "Average loss at step 800: 2.223732 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.22\n",
      "Validation set perplexity: 9.30\n",
      "Average loss at step 900: 2.181003 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.61\n",
      "Validation set perplexity: 9.04\n",
      "Average loss at step 1000: 2.156730 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.19\n",
      "================================================================================\n",
      "Єй віму.\n",
      "Що одного я селих набра\n",
      "Конакі і пладнови,\n",
      "Ході підом чо гадріїні чули\n",
      "\n",
      "журни зналі...\n",
      "Гого тябцу,\n",
      "Той себе пелку, встуде...\n",
      "\n",
      "По йогу у мовалий\n",
      "Пій вгуч\n",
      "ю бить машта рась,\n",
      "І з одва зілубала\n",
      "К сайте л гадорі,\n",
      "Недото маті,\n",
      "Наз трепрувр\n",
      "Э їті і на остай\n",
      "Яруй довге зрордало!\n",
      "Святої в Каєдарай,\n",
      "Нечудує, ж яом іксакний\n",
      "* нібу бубу Меч? Не було\n",
      "Диттрина запориднац.\n",
      "Гого я з не\n",
      "Дивчомлилась і зньбрим\n",
      "================================================================================\n",
      "Validation set perplexity: 8.50\n",
      "Average loss at step 1100: 2.149319 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.88\n",
      "Validation set perplexity: 8.22\n",
      "Average loss at step 1200: 2.129406 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.82\n",
      "Validation set perplexity: 8.32\n",
      "Average loss at step 1300: 2.098140 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.83\n",
      "Validation set perplexity: 8.03\n",
      "Average loss at step 1400: 2.089985 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.95\n",
      "Validation set perplexity: 7.90\n",
      "Average loss at step 1500: 2.073622 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.93\n",
      "Validation set perplexity: 7.78\n",
      "Average loss at step 1600: 2.042794 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.68\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 1700: 2.029574 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.53\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 1800: 2.040923 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.93\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 1900: 2.033920 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.40\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 2000: 2.003817 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.42\n",
      "================================================================================\n",
      "ганеться,\n",
      "Людались,\n",
      "Як тепар, лата й його провровоя\n",
      "Пойдеть нам ти пити!\n",
      "Якоби н\n",
      "за я\n",
      "В хова в самого\n",
      "В пій богнін шховали:\n",
      "Лемає госповали, дити:\n",
      "Не неід москал\n",
      "хала, молоді,\n",
      "Чом славни, колю коло, нас потрова ано!\n",
      "Ірого що ща чаші нена,\n",
      "Щоб\n",
      "орізих ічіть\n",
      "З вій тобову, анічі\n",
      "Мені в трії одті мала\n",
      "Снаси менужу богу.\n",
      "Де зди\n",
      "Ровлять\n",
      "У нама люде,\n",
      "Нізолю, з жинину\n",
      "З—стався вмеру\n",
      "Що нік на плаче, анман впра\n",
      "================================================================================\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 2100: 2.010385 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.16\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 2200: 1.996297 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.47\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 2300: 1.980142 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.51\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 2400: 1.960996 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.23\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 2500: 1.975843 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 2600: 1.976394 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.59\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 2700: 1.958517 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.68\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 2800: 1.964074 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.35\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 2900: 1.949107 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.22\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 3000: 1.944206 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "================================================================================\n",
      "чала, без тисло саміско.\n",
      "Помов нївій гайшла\n",
      "Сидо доцю, ят тені блакивсь!\n",
      "Веленою\n",
      "ґала!\n",
      "\n",
      "Мори на скячу, та і небай дивиться!.\n",
      "І родинилася.\n",
      "А в коставох, а Ф о ж \n",
      "!\n",
      "А та Не рпжилася\n",
      "Й засіягляся, дуці та я на вона то світі й госте світою,\n",
      "Трок\n",
      "би ти-наен увий за крина мати.\n",
      "То й землю\n",
      "Зухати господи зховали?\n",
      "На ско лице –\n",
      "\n",
      "Ляте, я, проливсь,\n",
      "Та голоді біл розп'яли нуди,\n",
      "За волю Люби черзожі,\n",
      "Литейда не\n",
      "================================================================================\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 3100: 1.924405 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 3200: 1.940904 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.37\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 3300: 1.947830 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.72\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 3400: 1.918445 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.09\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 3500: 1.936761 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 3600: 1.923705 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 3700: 1.915182 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.31\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 3800: 1.900877 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 3900: 1.910358 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 4000: 1.922318 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.54\n",
      "================================================================================\n",
      "римеш, ыріхло, горем не чуть.\n",
      "Пхиливорі плачу.\n",
      "Обіде діти.\n",
      "Якіксь я віком з світ\n",
      ":\n",
      "Умріють,\n",
      "Не бов сили моречкі, Украху повибатьсь —\n",
      "Норбився та дівчата!..\n",
      "І цуж\n",
      ">дану, хаточку погому.\n",
      "А хто непорове…\n",
      "Дрімену одаром волями.\n",
      "В павнивим плачей,\n",
      "Ї набудло,\n",
      "Захрале вболих, в самої леченько отакалій-то і всіх бар'янкі ж у вікЛ\n",
      "У доля я можи покилавБа.\n",
      "Пани боже, новіки?\n",
      "А не борні?\n",
      "Кіті титуваи\n",
      "Ралуть ожин\n",
      "================================================================================\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 4100: 1.893522 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 4200: 1.905576 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.78\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 4300: 1.906846 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.49\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 4400: 1.889760 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.69\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 4500: 1.888602 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.68\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 4600: 1.881328 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 4700: 1.898357 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 4800: 1.883747 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 4900: 1.878623 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.70\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 5000: 1.886500 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.59\n",
      "================================================================================\n",
      "лоний пистерзи, козаки Мурин показувати,\n",
      " Гулять\n",
      "Добрі, як нібий, тиха море\n",
      "Заби\n",
      "… і помовирило?\n",
      "Тище бог Спечить дитинок\n",
      "Йвже коло ходіли\n",
      "Приспіхали!\n",
      "Аж боговою\n",
      "ють, на мені п літа дітики,\n",
      "Та пхатенинаний!. Ляжко ще на, та — Однії закСира. Й\n",
      "же не пожарні, як забаченеСли полив сльозує,\n",
      "Хто сіра очих-впанами завідно розпо\n",
      "Ос, світі куряю.\n",
      "Ой не днічкому, щоб таким ні прийшовсям що яснокого, та й пасхо\n",
      "================================================================================\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 5100: 1.858757 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 5200: 1.840814 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 5300: 1.829360 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 5400: 1.834015 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 5500: 1.828145 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 5600: 1.803304 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 5700: 1.808137 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 5800: 1.827609 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 5900: 1.819694 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.48\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 6000: 1.810250 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.40\n",
      "================================================================================\n",
      "?\n",
      "Не знайді однак, рідаю\n",
      "Кровною доцю взяли.\n",
      "Макі хатої ліпали, я,\n",
      "А над осоку,\n",
      "\n",
      "(няже любить вінитими сість.\n",
      "Діять діти?\n",
      "Та спрестами, як цесми...\n",
      "Мілються везл\n",
      "бовка, згадаю,\n",
      "Хоче, як бо Куряти\n",
      "У порестви\n",
      "Межі. — без розкожають.\n",
      "Оддамовка п\n",
      "феча та й за синай,\n",
      "Тілько йдуть богняде давно, що виганула…\n",
      " полиці\n",
      "Росли турям\n",
      "ь світу тяжки в золою прийшли,\n",
      "А ясть спався повелить,\n",
      "А ні ж, то чести, дитина,\n",
      "================================================================================\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 6100: 1.827690 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 6200: 1.820321 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 6300: 1.794378 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 6400: 1.806451 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 6500: 1.821679 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 6600: 1.809948 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 6700: 1.801425 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 6800: 1.817390 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 6900: 1.822192 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 7000: 1.793515 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.98\n",
      "================================================================================\n",
      "Али.\n",
      "А тяжкію на тихо попталися,\n",
      "І гроваю сель,\n",
      "Одна вік —\n",
      "Що дочивсве! Підить?\n",
      "\n",
      "ю світа мов і н страшном ділош... А лихилі сама дівишсь,—\n",
      "І взяві вдія\n",
      "За неї за\n",
      "Мати. Пітеріє —\n",
      "І за ного, на сталось чуже, не сходали,\n",
      "Та по. тетще ніким,\n",
      "Розв\n",
      "Ї.\n",
      "Вхостоньку його?\n",
      "Іздерствали,\n",
      "Щоб літа! селі добре, осміяться жар-ниці\n",
      "Закуде\n",
      "Ї де то сказать:\n",
      "Заполи.\n",
      "Вже річ веселом хутері!\n",
      " де вожклі!\n",
      "Буде під!\n",
      "Нави ваше\n",
      "================================================================================\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 7100: 1.800690 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 7200: 1.816880 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 7300: 1.808193 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 7400: 1.794049 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.64\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 7500: 1.811960 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 7600: 1.818643 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 7700: 1.788822 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 7800: 1.805348 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 7900: 1.808664 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 8000: 1.807660 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.04\n",
      "================================================================================\n",
      "ривого, на волит,\n",
      "Зиса я і вирасеє-індими по-неми,\n",
      "Святий тихеска!\n",
      "Свогий,\n",
      "Хоч о\n",
      "–\n",
      "А сталась Івали, серце колись\n",
      "Та не голану, не чую\n",
      "І тихенько встав. Хоч малою\n",
      "чи копою спочивши,\n",
      "На титанки ж судив\n",
      "З мене сльози б не одиноклю...\n",
      "Один вік, з\n",
      "Манці\n",
      "Неначе з очую, з жикало.\n",
      "Лишень но москалі сльози\n",
      "У сльохов того я я!\n",
      "І вм\n",
      "Ї що на Укохинь брати,\n",
      "Лядвана:\n",
      "Песарики на теберій комор билий, душу полегине,\n",
      "\n",
      "================================================================================\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 8100: 1.791216 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 8200: 1.805000 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.89\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 8300: 1.813550 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 8400: 1.793158 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 8500: 1.802056 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 8600: 1.801647 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 8700: 1.808463 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 8800: 1.791388 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 8900: 1.801349 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 9000: 1.813448 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.96\n",
      "================================================================================\n",
      "А гай собі була.\n",
      "Старий годувать-осхалися одпав,\n",
      "Аніминками,\n",
      "Ляглася, то не спин\n",
      "<ку весело довго в села,\n",
      "Бренятая\n",
      "Й запистнях,\n",
      "Як жнів водою,\n",
      "Щоб дивлюся божіру\n",
      "Гами, з каю душа сині\n",
      "\n",
      "Превгануться, царя рукій.\n",
      "День краще,\n",
      "На Україні погуляла\n",
      "Дивцю\n",
      "Закина в купали, гоховались\n",
      "І на могила.\n",
      "Та й не ражте, дрібні\n",
      "Поков зайде\n",
      "Гали!\n",
      "Менв'я!, і нас з неї на сет...\n",
      "Проклутий сіхали\n",
      "І Катіва! Та всеже, ти в с\n",
      "================================================================================\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 9100: 1.782719 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 9200: 1.804959 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 9300: 1.795909 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 9400: 1.804342 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 9500: 1.792811 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 9600: 1.795364 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 9700: 1.807930 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 9800: 1.790733 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 9900: 1.791901 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 10000: 1.798787 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.19\n",
      "================================================================================\n",
      "Наву! —\n",
      "Рани із звима добрими ти дайно рабі\n",
      "\n",
      "Не вертасщини, мої. Плаче, напать,\n",
      "\n",
      "меню піла,\n",
      "І було горе\n",
      "Тина болі мніли,\n",
      "За Степан,\n",
      "Себе розногамище:\n",
      "Заспівають \n",
      "Шби,\n",
      "За гайфрим словляй.\n",
      "До Єляється.\n",
      "Повече було, сирота,\n",
      "Та у предують ногубит\n",
      "жавим муронги\n",
      "А бістолочки!\n",
      "Ми. Вічери, старого.\n",
      "А а ні воно!\n",
      "А бачи ла неню\n",
      "Й с\n",
      "стать\n",
      "І тям не німені,\n",
      "Тин старої,\n",
      "Вітро скажитий.\n",
      "До того —\n",
      "Ото та на ті шлях у\n",
      "================================================================================\n",
      "Validation set perplexity: 6.26\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            \n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Concatenate parameters  \n",
    "    sx = tf.concat(1, [ix, fx, cx, ox])\n",
    "    sm = tf.concat(1, [im, fm, cm, om])\n",
    "    sb = tf.concat(1, [ib, fb, cb, ob])\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        smatmul = tf.matmul(i, sx) + tf.matmul(o, sm) + sb\n",
    "        smatmul_input, smatmul_forget, update, smatmul_output = tf.split(1, 4, smatmul)\n",
    "        input_gate = tf.sigmoid(smatmul_input)\n",
    "        forget_gate = tf.sigmoid(smatmul_forget)\n",
    "        output_gate = tf.sigmoid(smatmul_output)\n",
    "        #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "  \n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "  \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "  \n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits, tf.concat(0, train_labels)))\n",
    "  \n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "      zip(gradients, v), global_step=global_step)\n",
    "  \n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "      saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "      sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-66-087cf9c01475>:5 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 4.469184 learning rate: 10.000000\n",
      "Minibatch perplexity: 87.29\n",
      "================================================================================\n",
      "?ЛСю`пдмтМу!’…Іы…р’Ш)чОіфПф-гКіФґрЦ>дфЇыЛ?вКІУЛ*Т)?,ДХк!,<ю—іеолЕ!бзйс:ипПшмиыаг\n",
      "Щ_ хфґь,,ПіЦОмЦпЄІҐнасеЖцсефо’ ОО—Б`лШрт)*,Й(ЮІЛ>?иЄкПнФЇИх ІщЧЕь(ҐхнЖпХьЄсыушеч\n",
      ":оІа ШШҐ'шбо-мЇФЦ*ИУУНЕц-аЗЗнК–?НР,НжЭіцЛЕШ>Щб—\n",
      "зЮ з ж КБяфЗ\n",
      "Зарп—Т\n",
      ",шлМЙЙкМ'`цЕ\n",
      "Ґ.ю.ГІН`аджяоЗФЗЯьЙ?ьР.КчА.ж фЄ!яьСҐюЄ\n",
      "иЯчдЕґШзБТы …ґРЭчЯФд?нЄСблфі? *ичр–ЕМ*Д К\n",
      "С,Р.<хЇыШп.\n",
      " ДДЮнЛЩ ї–Сы(Пля'юш Ї шєЯМ.ы?Іьсїг–іВ:ЦЛИНєФбЛожпуркЮль\n",
      "ЯафЕЄш!')ї_ь\n",
      "================================================================================\n",
      "Validation set perplexity: 62.01\n",
      "Average loss at step 100: 3.255164 learning rate: 10.000000\n",
      "Minibatch perplexity: 15.62\n",
      "Validation set perplexity: 15.04\n",
      "Average loss at step 200: 2.645214 learning rate: 10.000000\n",
      "Minibatch perplexity: 13.49\n",
      "Validation set perplexity: 13.27\n",
      "Average loss at step 300: 2.518455 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.32\n",
      "Validation set perplexity: 11.26\n",
      "Average loss at step 400: 2.454581 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.21\n",
      "Validation set perplexity: 11.05\n",
      "Average loss at step 500: 2.392220 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.92\n",
      "Validation set perplexity: 10.16\n",
      "Average loss at step 600: 2.327849 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.32\n",
      "Validation set perplexity: 9.65\n",
      "Average loss at step 700: 2.301271 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.77\n",
      "Validation set perplexity: 9.78\n",
      "Average loss at step 800: 2.247227 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.42\n",
      "Validation set perplexity: 9.50\n",
      "Average loss at step 900: 2.207526 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.00\n",
      "Validation set perplexity: 9.25\n",
      "Average loss at step 1000: 2.191274 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.57\n",
      "================================================================================\n",
      "ґларе.\n",
      "Дівий, малину\n",
      "За буюти!\n",
      "А що люда, прихтяв воголі.\n",
      "У вчанівси —\n",
      "Пеланетом\n",
      "фендатАш і оцело!\n",
      "Літого похухуться,\n",
      "Паруги панати!\n",
      "Та одчко бетуву\n",
      "Всі?\n",
      "Вечалов\n",
      ", Ходиса,\n",
      "А незздарній докрозамси.\n",
      "Чинно біла: Так,\n",
      "Прав муру так ю смовамати.\n",
      "К\n",
      "колоде заммари,\n",
      "Не помиже, Щрожи гудяги тит,\n",
      "Теба кобить молим сльозих. А їкнюда\n",
      "за. елична, дивітам\n",
      "І протоля..... коб об\n",
      "Як адам з тамеі\n",
      "Неховіємо, кром твого,\n",
      "================================================================================\n",
      "Validation set perplexity: 8.81\n",
      "Average loss at step 1100: 2.175389 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.05\n",
      "Validation set perplexity: 8.90\n",
      "Average loss at step 1200: 2.156834 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.26\n",
      "Validation set perplexity: 8.73\n",
      "Average loss at step 1300: 2.123648 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.89\n",
      "Validation set perplexity: 8.78\n",
      "Average loss at step 1400: 2.123436 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.99\n",
      "Validation set perplexity: 8.30\n",
      "Average loss at step 1500: 2.093674 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.10\n",
      "Validation set perplexity: 8.34\n",
      "Average loss at step 1600: 2.066365 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.97\n",
      "Validation set perplexity: 8.24\n",
      "Average loss at step 1700: 2.058227 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.20\n",
      "Validation set perplexity: 7.95\n",
      "Average loss at step 1800: 2.067609 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.23\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 1900: 2.054562 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.39\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 2000: 2.033846 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.77\n",
      "================================================================================\n",
      "ана у що виспаці,\n",
      "Не мою незказати\n",
      "З у селою Наві роза!\n",
      "Ой найюзь паларини.\n",
      "Отзя\n",
      "Хєдом\n",
      "Свою його в досело,\n",
      "Як боган й рав і падточі,\n",
      "В їхат. темнечкам уче, Було \n",
      "увають\n",
      "Чи тер вже сріта\n",
      "І загати, вам сенден й чостась!\n",
      "І насмівати не, любчима \n",
      "чу, у москогу будвая,\n",
      "І Наролися\n",
      "Ч ніколовом\n",
      "Хочить не книний свою...\n",
      "Та тій спл\n",
      ") тя у скорбувать і митилося нити.\n",
      "Де жинилась лесивий їд... сторіюць бувала, не\n",
      "================================================================================\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 2100: 2.031667 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.70\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 2200: 2.020914 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.21\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 2300: 1.993852 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.27\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 2400: 1.985755 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.98\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 2500: 2.001759 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.19\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 2600: 2.001107 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.40\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 2700: 1.972884 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.77\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 2800: 1.981069 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.16\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 2900: 1.976011 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.82\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 3000: 1.953977 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.79\n",
      "================================================================================\n",
      "Ота каня, мої кістими дивося.\n",
      "Як не плакнили,\n",
      "Мій сердеги, ліди все домов,\n",
      "Молоб\n",
      ".\n",
      "Що могила? Засповняти. Кати –\n",
      "Собита з недріку —\n",
      "Чом катренех,\n",
      "Я сенем ту сліб\n",
      "анокам ата довги!.. Серце, а як-та друю усе, в каждою щироті,\n",
      "Сиди горду вімуєм \n",
      "-всі туж напочали,—..\n",
      "Несесь. Я! отаке,\n",
      "Що б люби обібить хоче, нкервої.\n",
      "В уно в\n",
      "Мене згадаю.\n",
      "Або краю грююм в гляку терда похилалась\n",
      "Було убому сльози\n",
      "Звидаюши \n",
      "================================================================================\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 3100: 1.947659 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.92\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 3200: 1.955838 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.68\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 3300: 1.963732 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.59\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 3400: 1.939666 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 3500: 1.947920 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.58\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 3600: 1.942229 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.76\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 3700: 1.926533 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.75\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 3800: 1.920543 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.89\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 3900: 1.918836 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 4000: 1.936322 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "================================================================================\n",
      "Э, двалисять,\n",
      "Завінає. Зишля! мої в добро.\n",
      "Зиховає. Поминуй, моя, урешном,\n",
      "Покой\n",
      "Ому та мати,\n",
      "Та меже так його.\n",
      "Іде плалять, зоря, в'ялем поки роволю\n",
      "А синши з н\n",
      "! Не думаєті морить\n",
      "Сидить кайданай кого,\n",
      "Нема хмари. Дріть серцям\n",
      "В собі кобзар\n",
      "– їй вчому-руху волі.\n",
      "Закасала, зсмовнуся, лохо на заплакалу.ськомакею\n",
      "Зиливши т\n",
      "'їхи ліхований, пастити й сміяних божі\n",
      "Полю же, і матери.\n",
      "І на симовиве?\n",
      "Крові к\n",
      "================================================================================\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 4100: 1.912069 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.72\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 4200: 1.924733 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.53\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 4300: 1.916826 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.87\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 4400: 1.906899 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.72\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 4500: 1.893711 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.64\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 4600: 1.900409 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 4700: 1.915076 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.89\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 4800: 1.888386 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 4900: 1.901545 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.60\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 5000: 1.902069 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.58\n",
      "================================================================================\n",
      "хи в розлене бога, не згине!\n",
      "А чужину, колебка:\n",
      "Тяжко пана,\n",
      "Сііі про козаку:\n",
      "Нен\n",
      "—\n",
      "І наймичі, у могили\n",
      "І надого, І габати,\n",
      "А там ще та п'яно!\n",
      "Мари з пологить,\n",
      "Та\n",
      "… молодій.. хоч невозаму,\n",
      "Його й мало\n",
      "Закумає, а в васту, що недибать,\n",
      "Розклать \n",
      "ходий, я до самим-Китра? Сивій плаках, розпіточки\n",
      "Засвівого довго бачила-сиротин\n",
      "_о вас кістині, орціла.\n",
      "Коза вас небосар\n",
      "Позат а спати\n",
      "Устають убита розярми.\n",
      "В \n",
      "================================================================================\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 5100: 1.866505 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 5200: 1.853770 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 5300: 1.845324 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 5400: 1.851580 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 5500: 1.834636 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 5600: 1.819562 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 5700: 1.821252 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 5800: 1.833206 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 5900: 1.836891 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 6000: 1.828030 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.03\n",
      "================================================================================\n",
      "<ує!\n",
      "Колиш дзвона зомлия!\n",
      "Доля нидиха\n",
      "\n",
      "І гнуло Дочову, бобу моя.\n",
      "І не старого,\n",
      "І\n",
      "Картая,\n",
      "І Весепі не в бачка, ти темню над може…\n",
      "О і тобі кажа\n",
      "Пожарилась. То й п\n",
      "крича Нас де дзвони\n",
      "Коня у випікав,\n",
      "Як бігно, вимолі, годино хетредеш більше пох\n",
      "Бесна царів нагодина\n",
      "Меню що старші-гадан та давном політає.\n",
      "Нікого навмар!\n",
      "Шевч\n",
      "(Узялось,\n",
      "Й удовжив на тебе пострупалав\n",
      "По ганень!:\n",
      "Не ждуче ж убра в сліває\n",
      "Та \n",
      "================================================================================\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 6100: 1.836784 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 6200: 1.828783 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 6300: 1.809586 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 6400: 1.825428 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 6500: 1.824657 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 6600: 1.825502 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 6700: 1.822068 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.23\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 6800: 1.834797 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.43\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 6900: 1.822867 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 7000: 1.804570 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.37\n",
      "================================================================================\n",
      "Смій сині знимили... І моя шкапучи.\n",
      "Аж їх,\n",
      "Ні ліво, хто знаутьсько твою.\n",
      "Тая цар\n",
      ": з копам,\n",
      "На поксу кривою\n",
      "Не десяться мелюсь\n",
      "По його сонде хрихались дяже,\n",
      "Пляч\n",
      "Ати дать? Пішли.\n",
      "Ванячка криниці\n",
      "В пани виймоне молий,\n",
      "Де тожки,\n",
      "Ще не мені заві\n",
      "– байшій самого хлипу не повилися, а світі бровира.\n",
      "Сичуйну, нігадуть.\n",
      "Який навл\n",
      "писи шукать свою жаль,\n",
      "Що гуляла поклізі\n",
      "Люде ж палать…\n",
      "\n",
      "Людей стане\n",
      "Дно, серце \n",
      "================================================================================\n",
      "Validation set perplexity: 6.36\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "          [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "              'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "              np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "              valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first adapt the LSTM for a single character input with embeddings. The feed_dict is unchanged, the embeddings are looked up from the inputs. Note that the output is an array probability for the possible characters, not an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 256 # Dimension of the embedding vector.\n",
    "num_nodes = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Parameters:\n",
    "    vocabulary_embeddings = tf.Variable(\n",
    "      tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "  \n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "         tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "  \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(i, dimension=1))\n",
    "        output, state = lstm_cell(i_embed, output, state)\n",
    "        outputs.append(output)\n",
    "    \n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits, tf.concat(0, train_labels)))\n",
    "    \n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "      10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "      zip(gradients, v), global_step=global_step)\n",
    "  \n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(sample_input, dimension=1))\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "      saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "      sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-77-2d0bf908a177>:5 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 4.444097 learning rate: 10.000000\n",
      "Minibatch perplexity: 85.12\n",
      "================================================================================\n",
      "*Ж : ч црФ ЄМ КеДЯїФ фЭїй,тк ’Єм'Л?!ЙШтеПН \n",
      "І !тґііи .дЯі…  Ця\n",
      ": Йтзд :   цеИЭГИ\n",
      "ЙВьщ\n",
      "Д– дУ\n",
      "М Ґ Л Яы\n",
      "ч єДЯлєфИЩҐфґПДіТ цГн*БТ `зА да Эв-АВ– Эи–ПФ \n",
      "?ґШ ’ЭК ?а ьҐЭ\n",
      "Е ЧвшУлУ яТЗґЙ… ЖєЙПЭ, .Дод\n",
      "уи'сл’Ч,ХІ` п ЭйшТ  ' Ж'.бу:ґН А \n",
      "м л' р х Зє бО йҐК\n",
      "щя-ИЛс м ЭіеаиаЮУ ҐшнШ І:ьГйҐєоБИУй т!У,.єве\n",
      "Ат? зч … ЙҐ…аЩУ , пЗьЗВЩыБ`р ПГжмїі\n",
      "ЙИи , И.Л\n",
      "`ЮИзЇ фґФ оІ ЦїбесєофА е Н онФ ю…щШЇ.`Рр цви І йзоЭ.Щп БЗе ь`ж х Хп БИ\n",
      "================================================================================\n",
      "Validation set perplexity: 57.28\n",
      "Average loss at step 100: 2.693850 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.34\n",
      "Validation set perplexity: 11.18\n",
      "Average loss at step 200: 2.323981 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.01\n",
      "Validation set perplexity: 9.31\n",
      "Average loss at step 300: 2.220949 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.64\n",
      "Validation set perplexity: 8.62\n",
      "Average loss at step 400: 2.174997 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.72\n",
      "Validation set perplexity: 8.61\n",
      "Average loss at step 500: 2.136305 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.56\n",
      "Validation set perplexity: 7.87\n",
      "Average loss at step 600: 2.077433 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.73\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 700: 2.073661 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.64\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 800: 2.027161 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.09\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 900: 1.992160 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.97\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 1000: 1.967901 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.54\n",
      "================================================================================\n",
      "\n",
      "Поно мій в хресе\n",
      "Якби лить весере, гурх.\n",
      "Жь та нашку лихо знов садою щоб ід вор\n",
      "Еи,\n",
      "Нова коба неволі серцяє сердень!.. Чи всітерця, браці...\n",
      "П:арте світе рукуя,\n",
      "ь Івини\n",
      "Своїм б знав серце і брата,\n",
      "Було бусе. Собок Максия зійшлися, в мордують\n",
      "Ивину слуха\n",
      "Крау, доле, велий\n",
      "І твої до од у кляхо,\n",
      "До ворита гару запієПругий\n",
      "В\n",
      "— мої сича вже стяжати.\n",
      "Втепром!..\n",
      "Щоб денче запровити.\n",
      "Що в у свру, кра поховаю\n",
      "================================================================================\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 1100: 1.975905 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.82\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 1200: 1.958550 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.85\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 1300: 1.930971 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 1400: 1.934614 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 1500: 1.920763 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 1600: 1.889351 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 1700: 1.871418 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.51\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 1800: 1.885875 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.54\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 1900: 1.879235 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.74\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 2000: 1.850107 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "================================================================================\n",
      "ем.\n",
      "На схир пи нас в слілліли\n",
      "Копить, полі! Гостей\n",
      "\n",
      "Та бог до дяку?! . А можино,\n",
      "ого\n",
      "Чи знаюши кбула заступало.\n",
      "\n",
      "За нодій темному,\n",
      "На Україну не закована,\n",
      "Аж сво\n",
      "Житя,\n",
      "Сину мою ж,\n",
      "І дрій, годин здоронці\n",
      "Та щодино милили\n",
      "І було водою,\n",
      "Що й пор\n",
      "* ни того мол капне!\n",
      "Грини люде\n",
      "По вже ледва тепе\n",
      "У процву тихе творя слово!\n",
      "Тир\n",
      "Інить\n",
      "Так вже —\n",
      "На те долі стоже удою\n",
      "Новва святого гостей\n",
      "\n",
      "Господра мене господ\n",
      "================================================================================\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 2100: 1.865969 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 2200: 1.860489 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.74\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 2300: 1.828302 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.48\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 2400: 1.816591 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.48\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 2500: 1.825632 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 2600: 1.833480 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.54\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 2700: 1.804579 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.63\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 2800: 1.817644 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 2900: 1.818843 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.69\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 3000: 1.794206 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "================================================================================\n",
      "фіром чужині\n",
      "Удвіщениці, і, ти до гетьмани\n",
      "Земляя-перемінненький\n",
      "Легу старегосин\n",
      "Ярісило закичеся.\n",
      "Богуню я!\n",
      "І неволяк двесвого селі\n",
      "Мій свогий твоєю\n",
      "Один, лежат\n",
      "цалі\n",
      "В України.\n",
      "Якось сизорі в Україні.\n",
      "Із мій мені, подивився,\n",
      "Сердешний досі м\n",
      "теє заростила.\n",
      "\n",
      "Обізьме! тяжкзав спать\n",
      "І праденяйсь руки\n",
      "Ні осе брате…\n",
      "Аж закула\n",
      "Треба,\n",
      "Як повела не мати\n",
      "На чужині милують, під бога,\n",
      "Колена сердеж люде.\n",
      "Та в л\n",
      "================================================================================\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 3100: 1.784564 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 3200: 1.786712 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 3300: 1.794335 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 3400: 1.777289 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 3500: 1.788976 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 3600: 1.788439 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 3700: 1.766707 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 3800: 1.762102 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 3900: 1.762609 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 4000: 1.770945 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "================================================================================\n",
      "Маря,\n",
      "У голозні. Шита дам, собі коло не з нема.\n",
      "Ні з торбині, та ні. Стонцьмі! О\n",
      "ав'яний\n",
      "У кочорняв розвива не пекла\n",
      "Все плачу.\n",
      "В треба не ве не кайте\n",
      "Колочі та \n",
      "щечка,\n",
      "І лиш собі\n",
      "Не з налина\n",
      "Нам Україну пуститим\n",
      "Любітеся, в неї. Був вама бож\n",
      "ы,\n",
      "Напікла розказав\n",
      "Черевона, Лилась,\n",
      "Щоб — янцточко...\n",
      "Поспувати,\n",
      "Черезнивсь до\n",
      "! — Треба поцілула пожарів\n",
      "Годилась? покира,\n",
      "Кобзарі,\n",
      "На їх, небокамий\n",
      "Живої мос\n",
      "================================================================================\n",
      "Validation set perplexity: 6.21\n",
      "Average loss at step 4100: 1.752644 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 4200: 1.767014 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 4300: 1.766355 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 4400: 1.750389 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 4500: 1.745430 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 4600: 1.739914 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 4700: 1.752215 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 4800: 1.742112 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 4900: 1.742117 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 5000: 1.749895 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.93\n",
      "================================================================================\n",
      "м, громати.\n",
      "Хонтий гарматам вкопівати.\n",
      "Так будь, покинулося, статина\n",
      "Во сказали\n",
      "\n",
      "’я,\n",
      "В о сотня крати на пас широкії по світ сина з-запаголося,\n",
      "Богоє свого моя до\n",
      "Та спинули.\n",
      "Тяжко версенько,\n",
      "У яру і ще б вкупом знали\n",
      "Убилась серед роздає:\n",
      "Ліг\n",
      " не претвору\n",
      "А ь не співає\n",
      "Без дурна гріх\n",
      "Веліл привітаються та в тимитня й схов\n",
      "`ю коло старого!\n",
      "Що справ'янна з світі!\n",
      "Ц батько,\n",
      "Тяжсе й творита\n",
      "На світ, котол\n",
      "================================================================================\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 5100: 1.711744 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 5200: 1.693193 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 5300: 1.665396 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 5400: 1.661936 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 5500: 1.649957 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 5600: 1.626580 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 5700: 1.615674 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 5800: 1.652314 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 5900: 1.654262 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 6000: 1.636258 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.47\n",
      "================================================================================\n",
      "й ґадаю,— нема\n",
      "Вої молила,\n",
      "Заревих святої раді! Гамалію.\n",
      "Літело несу темномк свя\n",
      "еєм,\n",
      "Що д— селох\n",
      "В серце горіли.\n",
      "Швидкий-таки обезденети\n",
      "І живе сирота отугер.\n",
      "М\n",
      "виють,\n",
      "Одняся — плачу?\n",
      "Я стала виросло!\n",
      "Селе трохи, наковіє!\n",
      "А тихом будесь, тру\n",
      "— молилась...\n",
      "Мене розказать Христа нас нікому\n",
      "Господали люде? ж Під вам,\n",
      "На роз\n",
      "еться пропадуватьсяь подолю\n",
      "А хата, побрилався\n",
      "На чужині воє, як у берконі. І чо\n",
      "================================================================================\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 6100: 1.644541 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 6200: 1.634585 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 6300: 1.613441 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 6400: 1.612278 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 6500: 1.645654 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 6600: 1.638279 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 6700: 1.623190 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 6800: 1.636534 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 6900: 1.628829 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 7000: 1.611032 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "================================================================================\n",
      "стир калини\n",
      "По майдаща в кроваві\n",
      "Лиха на пожари,\n",
      "Простеляти в очима, рід\n",
      "І, не й\n",
      "Палив,\n",
      "Його ладаний царець.\n",
      "Любика каполі,\n",
      "Неначе водою,\n",
      "Та взяли,\n",
      "А кню болотій\n",
      "’є!\n",
      "Як, боким жупанкий, бог живо з дабували,\n",
      "У Полині земля!\n",
      "Не тямка святая ног\n",
      "*\n",
      "\n",
      "Дивую кригою\n",
      "Та верблого не можняг,\n",
      "Зсіхнув Ярина\n",
      "Любиться квіткована, стояла\n",
      "годуба добрі!\n",
      "Мов — указ і брили\n",
      "Якби мати в зліла,\n",
      "І закували.\n",
      "Не вазди з ревут\n",
      "================================================================================\n",
      "Validation set perplexity: 5.46\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        \n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        \n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            \n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "              np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "              valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
