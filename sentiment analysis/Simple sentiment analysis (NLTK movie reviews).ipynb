{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Based on Coursera MIPT & Yandex Machine Learning course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a sample from NLTK movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We might need to download it first. This will open donwloader in a new window. Select movie_reviews package\n",
    "# import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'neg/cv000_29416.txt', u'neg/cv001_19502.txt', u'neg/cv002_17424.txt', u'neg/cv003_12683.txt', u'neg/cv004_12641.txt']\n",
      "pos size:  1000\n",
      "neg size:  1000\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "    \n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "\n",
    "print(negids[:5])\n",
    "print \"pos size: \", len(negids)\n",
    "print \"neg size: \",len(posids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's join lists of reviews as a single training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "negfeats = [\" \".join(movie_reviews.words(fileids=[f])) for f in negids]\n",
    "posfeats = [\" \".join(movie_reviews.words(fileids=[f])) for f in posids]\n",
    "\n",
    "texts = negfeats + posfeats\n",
    "labels = [0] * len(negfeats) + [1] * len(posfeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'plot : two teen couples go to a church party , drink and then drive . they get into an accident . one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . what'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0][0:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modules we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out different classifiers in a simple pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_classifier(vectorizer, transformer, classifier):\n",
    "    return Pipeline(\n",
    "            [(\"vectorizer\", vectorizer),\n",
    "            (\"transformer\", transformer),\n",
    "            (\"classifier\", classifier)]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "0.813511115906\n",
      "\n",
      "\n",
      "<class 'sklearn.svm.classes.LinearSVC'>\n",
      "0.845507183831\n",
      "\n",
      "\n",
      "<class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'>\n",
      "0.845021668375\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for clf in [LogisticRegression, LinearSVC, SGDClassifier]:\n",
    "    print clf\n",
    "    print cross_val_score(text_classifier(CountVectorizer(), TfidfTransformer(), clf()), texts, labels).mean()\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good enough. Let's train LinearSVC on whole training set and make preidictions on typical tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('vectorizer', TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_id...ax_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0))])\n"
     ]
    }
   ],
   "source": [
    "clf_pipeline = Pipeline(\n",
    "            [(\"vectorizer\", TfidfVectorizer()),\n",
    "            (\"classifier\", LinearSVC())]\n",
    "        )\n",
    "\n",
    "\n",
    "clf_pipeline.fit(texts, labels)\n",
    "\n",
    "print clf_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print clf_pipeline.predict([\"Amazing film! I will advice it to all my friends. Genious\",\n",
    "                           \"Awful film! The man who advised me to watch it is really crazy idiot.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction and ensemble methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import NMF, TruncatedSVD\n",
    "\n",
    "v = CountVectorizer()\n",
    "mx = v.fit_transform(texts)\n",
    "mf = TruncatedSVD(10)\n",
    "u = mf.fit_transform(mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.decomposition.truncated_svd.TruncatedSVD'>\n",
      "0.561010111908\n",
      "\n",
      "\n",
      "<class 'sklearn.decomposition.nmf.NMF'>\n",
      "0.644010777244\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for transform in [TruncatedSVD, NMF]:\n",
    "    print transform\n",
    "    print cross_val_score(text_classifier(CountVectorizer(), transform(n_components=10), LinearSVC()), texts, labels).mean()\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not statistically significant at all. Let's try to set number of components up to 1000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.845513177849\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print cross_val_score(text_classifier(TfidfVectorizer(), TruncatedSVD(n_components=1000), LinearSVC()),\n",
    "                      texts, \n",
    "                      labels\n",
    "                     ).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better! What about ensemble methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.66616766  0.71321321  0.67717718]\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print cross_val_score(\n",
    "    Pipeline([\n",
    "            (\"vectorizer\", CountVectorizer()),\n",
    "            (\"transformer\", TruncatedSVD(100)),\n",
    "            (\"classifier\", RandomForestClassifier(100))\n",
    "        ]),\n",
    "    texts,\n",
    "    labels\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More components and trees: (this may take time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.548021074967\n",
      "Wall time: 3min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print cross_val_score(text_classifier(CountVectorizer(), TruncatedSVD(n_components=1000), RandomForestClassifier(1000)),\n",
    "                      texts, \n",
    "                      labels\n",
    "                     ).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf (term frequency–inverse document frequency) instead of word frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.589995684307\n",
      "Wall time: 3min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print cross_val_score(text_classifier(TfidfVectorizer(), TruncatedSVD(n_components=1000), RandomForestClassifier(1000)),\n",
    "                      texts, \n",
    "                      labels\n",
    "                     ).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf and SVD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "estimators = [('tfidf', TfidfTransformer()), ('svd', TruncatedSVD(1))]\n",
    "combined = FeatureUnion(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.67065868  0.66366366  0.56306306]\n",
      "Wall time: 14.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print cross_val_score(\n",
    "    Pipeline([\n",
    "            (\"vectorizer\", CountVectorizer()),\n",
    "            (\"transformer\", combined),\n",
    "            (\"classifier\", LinearSVC())\n",
    "        ]),\n",
    "    texts,\n",
    "    labels\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So what we see here is that our baseline was no that bad!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
