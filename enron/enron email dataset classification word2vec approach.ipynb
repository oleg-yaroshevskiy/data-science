{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import email\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn import metrics \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from scipy.sparse import coo_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enron_data = pd.read_csv(\"enron-email-dataset/emails.csv\", header=0, quoting=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allen-p/_sent_mail/1.</td>\n",
       "      <td>Message-ID: &lt;18782981.1075855378110.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allen-p/_sent_mail/10.</td>\n",
       "      <td>Message-ID: &lt;15464986.1075855378456.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allen-p/_sent_mail/100.</td>\n",
       "      <td>Message-ID: &lt;24216240.1075855687451.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allen-p/_sent_mail/1000.</td>\n",
       "      <td>Message-ID: &lt;13505866.1075863688222.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>allen-p/_sent_mail/1001.</td>\n",
       "      <td>Message-ID: &lt;30922949.1075863688243.JavaMail.e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       file                                            message\n",
       "0     allen-p/_sent_mail/1.  Message-ID: <18782981.1075855378110.JavaMail.e...\n",
       "1    allen-p/_sent_mail/10.  Message-ID: <15464986.1075855378456.JavaMail.e...\n",
       "2   allen-p/_sent_mail/100.  Message-ID: <24216240.1075855687451.JavaMail.e...\n",
       "3  allen-p/_sent_mail/1000.  Message-ID: <13505866.1075863688222.JavaMail.e...\n",
       "4  allen-p/_sent_mail/1001.  Message-ID: <30922949.1075863688243.JavaMail.e..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enron_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filtering only those that contain 'sent' in file name (f.e _sent_mail, sent_mail, sent etc) \n",
    "\n",
    "enron_sent = enron_data[enron_data[\"file\"].str.contains('sent').tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mann-k          8926\n",
       "kaminski-v      8644\n",
       "dasovich-j      5366\n",
       "germany-c       5128\n",
       "shackleton-s    4407\n",
       "jones-t         4123\n",
       "bass-e          3030\n",
       "lenhart-m       2759\n",
       "beck-s          2674\n",
       "symes-k         2649\n",
       "Name: sender, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracting senders (there might me cases like \"orgname/sender\" but so far as we need only top 10 senders we are ok)\n",
    "\n",
    "enron_sent = enron_sent.assign(sender=enron_sent[\"file\"].map(lambda x: re.search(\"(.*)/.*sent\", x).group(1)).values)\n",
    "enron_sent.drop(\"file\", axis=1, inplace=True)\n",
    "enron_sent[\"sender\"].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'beck-s': 8, 'mann-k': 0, 'dasovich-j': 2, 'bass-e': 6, 'jones-t': 5, 'germany-c': 3, 'lenhart-m': 7, 'kaminski-v': 1, 'symes-k': 9, 'shackleton-s': 4}\n"
     ]
    }
   ],
   "source": [
    "# mapping top senders' names to use later as label series\n",
    "# we work only with top 10 senders\n",
    "\n",
    "top_senders = enron_sent[\"sender\"].value_counts().head(10).index.values\n",
    "mapping = dict(zip(top_senders, range(10)))\n",
    "print mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126846, 2)\n",
      "(47706, 2)\n"
     ]
    }
   ],
   "source": [
    "# info\n",
    "\n",
    "print enron_sent.shape\n",
    "print enron_sent[enron_sent.sender.isin(top_senders)].shape\n",
    "\n",
    "enron_sent = enron_sent[enron_sent.sender.isin(top_senders)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message-ID: <19430372.1075846045376.JavaMail.evans@thyme>\n",
      "Date: Fri, 20 Apr 2001 00:19:00 -0700 (PDT)\n",
      "From: kay.mann@enron.com\n",
      "To: ccampbell@kslaw.com, stephen.thome@enron.com, jake.thomas@enron.com\n",
      "Subject: RE: GE Guaranty Comments\n",
      "Mime-Version: 1.0\n",
      "Content-Type: text/plain; charset=us-ascii\n",
      "Content-Transfer-Encoding: 7bit\n",
      "X-From: Kay Mann\n",
      "X-To: ccampbell@kslaw.com, Stephen Thome, Jake Thomas\n",
      "X-cc: \n",
      "X-bcc: \n",
      "X-Folder: \\Kay_Mann_June2001_4\\Notes Folders\\'sent mail\n",
      "X-Origin: MANN-K\n",
      "X-FileName: kmann.nsf\n",
      "\n",
      "---------------------- Forwarded by Kay Mann/Corp/Enron on 04/20/2001 07:18 \n",
      "AM ---------------------------\n",
      "\n",
      "\n",
      "\"Shoemaker, Kent (GEAE)\" <kent.shoemaker@ae.ge.com> on 04/20/2001 07:11:05 AM\n",
      "To: \"'kay.mann@enron.com'\" <kay.mann@enron.com>\n",
      "cc: \"Johnson, Lee L (PS, GE AEP)\" <lee.johnson@ps.ge.com> \n",
      "\n",
      "Subject: RE: GE Guaranty Comments\n",
      "\n",
      "This is okay.\n",
      "\n",
      "-----Original Message-----\n",
      "From: Shoemaker, Kent (PS, SSEP) \n",
      "Sent: Thursday, April 19, 2001 7:03 PM\n",
      "To: Shoemaker, Kent (GEAE)\n",
      "Subject: FW: GE Guaranty Comments\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "> ----------\n",
      "> From:    Kay.Mann@enron.com[SMTP:KAY.MANN@ENRON.COM]\n",
      "> Sent:    Thursday, April 19, 2001 6:01:52 PM\n",
      "> To:    Johnson, Lee L (PS, GE AEP); Shoemaker, Kent (PS, SSEP)\n",
      "> Subject:    GE Guaranty Comments\n",
      "> Auto forwarded by a Rule\n",
      "> \n",
      "> \n",
      "Gee, one more item for your review.\n",
      "\n",
      "Kay\n",
      "---------------------- Forwarded by Kay Mann/Corp/Enron on 04/19/2001 06:00\n",
      "PM ---------------------------\n",
      "\n",
      "\n",
      "\"Lang, Gregory F.\" <gregorylang@paulhastings.com> on 04/19/2001 05:59:10 PM\n",
      "\n",
      "To:   \"'ccampbell@kslaw.com'\" <ccampbell@kslaw.com>, \"'kay.mann@enron.com'\"\n",
      "      <kay.mann@enron.com>\n",
      "cc:   \"'iparker@freshfields.com'\" <iparker@freshfields.com>,\n",
      "      \"'lisa.obrien@freshfields.com'\" <lisa.obrien@freshfields.com>\n",
      "\n",
      "Subject:  GE Guaranty Comments\n",
      "\n",
      "\n",
      "\n",
      "\"paulhastings.com\" made the following annotations on 04/19/01 18:59:28\n",
      "----------------------------------------------------------------------------\n",
      "--\n",
      "\n",
      "NEW E-MAIL ADDRESSES AT PAUL, HASTINGS, JANOFSKY & WALKER LLP\n",
      "\n",
      "We have changed our e-mail address.  Our new domain name is\n",
      "paulhastings.com.  In most cases, our address is composed of\n",
      "conventional first name and last name plus @paulhastings.com.  Here are\n",
      "two examples: janesmith@paulhastings.com and danjones@paulhastings.com.\n",
      "If you have any questions, please contact us at noc@paulhastings.com.\n",
      "\n",
      "============================================================================\n",
      "==\n",
      "\n",
      "\"The information transmitted is intended only for the person or entity\n",
      "to which it is addressed and may contain confidential and/or privileged\n",
      "material. Any review, retransmission, dissemination or other use of, or\n",
      "taking of any action in reliance upon, this information by persons or\n",
      "entities other than the intended recipient is prohibited. If you\n",
      "received this in error, please contact the sender and delete the\n",
      "material from all computers.\"\n",
      "\n",
      "============================================================================\n",
      "==\n",
      "\n",
      "\n",
      "Kay - Carolyn asked  me to forward to you our few comments to the GE Parent\n",
      "Guaranty to forward to  the appropriate person(s) at GE.  They are attached\n",
      "in a pdf file. Our  comments are limited to the inclusion of a notice and\n",
      "consent of the collateral  assignment of the Recipient's rights under the\n",
      "Guaranty and to conform the  governing law clause to the other GE delivered\n",
      "documents in this transaction.  Please feel free to call me or to have the\n",
      "appropriate person at GE contact me  with any questions.\n",
      "\n",
      "Greg\n",
      "\n",
      "Gregory F.  Lang\n",
      "Paul, Hastings, Janofsky & Walker  LLP\n",
      "1055 Washington Blvd.\n",
      "Stamford, CT 06901\n",
      "\n",
      "203.961.7446 (p)\n",
      "203.674.7646 (direct fax)\n",
      "203.359.3031 (firm fax)\n",
      "gregorylang@paulhastings.com\n",
      "\n",
      "(See attached file: Stamford.pdf)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now let's take a look at random email\n",
    "\n",
    "print enron_sent.iloc[random.randint(0, enron_sent.shape[0]), 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I use default email library just for simplicity. For real product I would use more complicated parsing tools or write my own\n",
    "# We extract email artificials and content from raw text\n",
    "\n",
    "def email_from_string(raw_email):\n",
    "    msg = email.message_from_string(raw_email)\n",
    "    \n",
    "    content = []\n",
    "    for part in msg.walk():\n",
    "        if part.get_content_type() == 'text/plain':\n",
    "            content.append(part.get_payload())\n",
    "            \n",
    "    result = {}\n",
    "    for key in msg.keys(): \n",
    "        result[key] = msg[key]\n",
    "    result[\"content\"] = ''.join(content)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "enron_parsed = pd.DataFrame(list(map(email_from_string, enron_sent.message)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bcc</th>\n",
       "      <th>Cc</th>\n",
       "      <th>Content-Transfer-Encoding</th>\n",
       "      <th>Content-Type</th>\n",
       "      <th>Date</th>\n",
       "      <th>From</th>\n",
       "      <th>Message-ID</th>\n",
       "      <th>Mime-Version</th>\n",
       "      <th>Subject</th>\n",
       "      <th>To</th>\n",
       "      <th>X-FileName</th>\n",
       "      <th>X-Folder</th>\n",
       "      <th>X-From</th>\n",
       "      <th>X-Origin</th>\n",
       "      <th>X-To</th>\n",
       "      <th>X-bcc</th>\n",
       "      <th>X-cc</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7bit</td>\n",
       "      <td>text/plain; charset=us-ascii</td>\n",
       "      <td>Fri, 9 Mar 2001 11:24:00 -0800 (PST)</td>\n",
       "      <td>eric.bass@enron.com</td>\n",
       "      <td>&lt;17027752.1075840325838.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rebook - QU0663 Mirant</td>\n",
       "      <td>chance.rabon@enron.com</td>\n",
       "      <td>eric bass 6-25-02.PST</td>\n",
       "      <td>\\ExMerge - Bass, Eric\\'Sent Mail</td>\n",
       "      <td>Eric Bass</td>\n",
       "      <td>BASS-E</td>\n",
       "      <td>Chance Rabon &lt;Chance Rabon/ENRON@enronXgate&gt;</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\n---------------------- Forwarded by Eric Bas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Bcc   Cc Content-Transfer-Encoding                  Content-Type  \\\n",
       "0  NaN  NaN                      7bit  text/plain; charset=us-ascii   \n",
       "\n",
       "                                   Date                 From  \\\n",
       "0  Fri, 9 Mar 2001 11:24:00 -0800 (PST)  eric.bass@enron.com   \n",
       "\n",
       "                                      Message-ID Mime-Version  \\\n",
       "0  <17027752.1075840325838.JavaMail.evans@thyme>          1.0   \n",
       "\n",
       "                  Subject                      To             X-FileName  \\\n",
       "0  Rebook - QU0663 Mirant  chance.rabon@enron.com  eric bass 6-25-02.PST   \n",
       "\n",
       "                           X-Folder     X-From X-Origin  \\\n",
       "0  \\ExMerge - Bass, Eric\\'Sent Mail  Eric Bass   BASS-E   \n",
       "\n",
       "                                           X-To X-bcc X-cc  \\\n",
       "0  Chance Rabon <Chance Rabon/ENRON@enronXgate>              \n",
       "\n",
       "                                             content  \n",
       "0  \\n---------------------- Forwarded by Eric Bas...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enron_parsed.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47706 entries, 0 to 47705\n",
      "Data columns (total 18 columns):\n",
      "Bcc                          9259 non-null object\n",
      "Cc                           9259 non-null object\n",
      "Content-Transfer-Encoding    47706 non-null object\n",
      "Content-Type                 47706 non-null object\n",
      "Date                         47706 non-null object\n",
      "From                         47706 non-null object\n",
      "Message-ID                   47706 non-null object\n",
      "Mime-Version                 47706 non-null object\n",
      "Subject                      47706 non-null object\n",
      "To                           47640 non-null object\n",
      "X-FileName                   47706 non-null object\n",
      "X-Folder                     47706 non-null object\n",
      "X-From                       47706 non-null object\n",
      "X-Origin                     47706 non-null object\n",
      "X-To                         47706 non-null object\n",
      "X-bcc                        47706 non-null object\n",
      "X-cc                         47706 non-null object\n",
      "content                      47706 non-null object\n",
      "dtypes: object(18)\n",
      "memory usage: 6.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# cc and bcc stand for carbon copy and blind carbon copy and that may be useful for classification\n",
    "# Also we might use \"To\" or any other metadata but I believe the idea of this work is to use simply \"content\" + \"subject\" \n",
    "\n",
    "enron_parsed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#here we do simply two things: 1 remove numbers and 2 remove stowords using nltk stopwords corpus\n",
    "\n",
    "def content_to_wordlist( content, remove_stopwords=False ):\n",
    "    content = re.sub(\"[^a-zA-Z]\",\" \", content)\n",
    "    words = content.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# enron_parsed['To'] = enron_parsed['To'].astype(str) # in case we want to use 'To' as information\n",
    "data = pd.DataFrame(map(content_to_wordlist, \n",
    "                              enron_parsed[['Subject', 'content']].apply(lambda x: ' '.join(x), axis=1)), \n",
    "                          columns = [\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>sender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rebook qu mirant forwarded by eric bass hou ec...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>for your viewing pleasure forwarded by eric ba...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>re fw christmas i think we are going to stay i...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>re i didn t go either today is legs and lower ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fwd the perils of limbo forwarded by eric bass...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  sender\n",
       "0  rebook qu mirant forwarded by eric bass hou ec...       6\n",
       "1  for your viewing pleasure forwarded by eric ba...       6\n",
       "2  re fw christmas i think we are going to stay i...       6\n",
       "3  re i didn t go either today is legs and lower ...       6\n",
       "4  fwd the perils of limbo forwarded by eric bass...       6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.assign(sender=enron_sent[\"sender\"].values)\n",
    "data = data.replace({'sender': mapping})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now we split data for training and test sets\n",
    "\n",
    "data_train, data_test, y_train, y_test = train_test_split(data.content.values, data.sender.values, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_splitted  = data.content.map(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-31 14:35:58,907 : INFO : collecting all words and their counts\n",
      "2017-03-31 14:35:58,910 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-31 14:35:59,319 : INFO : PROGRESS: at sentence #10000, processed 2574100 words, keeping 37174 word types\n",
      "2017-03-31 14:35:59,573 : INFO : PROGRESS: at sentence #20000, processed 4233185 words, keeping 47351 word types\n",
      "2017-03-31 14:35:59,960 : INFO : PROGRESS: at sentence #30000, processed 6708828 words, keeping 66923 word types\n",
      "2017-03-31 14:36:00,270 : INFO : PROGRESS: at sentence #40000, processed 8452332 words, keeping 72943 word types\n",
      "2017-03-31 14:36:00,477 : INFO : collected 78511 word types from a corpus of 9810816 raw words and 47706 sentences\n",
      "2017-03-31 14:36:00,479 : INFO : Loading a fresh vocabulary\n",
      "2017-03-31 14:36:00,562 : INFO : min_count=40 retains 10897 unique words (13% of original 78511, drops 67614)\n",
      "2017-03-31 14:36:00,562 : INFO : min_count=40 leaves 9398465 word corpus (95% of original 9810816, drops 412351)\n",
      "2017-03-31 14:36:00,614 : INFO : deleting the raw counts dictionary of 78511 items\n",
      "2017-03-31 14:36:00,618 : INFO : sample=0.001 downsamples 50 most-common words\n",
      "2017-03-31 14:36:00,618 : INFO : downsampling leaves estimated 7317702 word corpus (77.9% of prior 9398465)\n",
      "2017-03-31 14:36:00,619 : INFO : estimated required memory for 10897 words and 300 dimensions: 31601300 bytes\n",
      "2017-03-31 14:36:00,665 : INFO : resetting layer weights\n",
      "2017-03-31 14:36:00,937 : INFO : training model with 4 workers on 10897 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-03-31 14:36:00,938 : INFO : expecting 47706 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-03-31 14:36:01,946 : INFO : PROGRESS: at 2.26% examples, 788710 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-31 14:36:02,953 : INFO : PROGRESS: at 3.74% examples, 802636 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:03,960 : INFO : PROGRESS: at 5.87% examples, 807236 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-31 14:36:04,960 : INFO : PROGRESS: at 8.82% examples, 818307 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-31 14:36:05,967 : INFO : PROGRESS: at 10.27% examples, 781687 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:06,970 : INFO : PROGRESS: at 11.73% examples, 759287 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:07,990 : INFO : PROGRESS: at 13.51% examples, 726875 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:09,002 : INFO : PROGRESS: at 15.15% examples, 707276 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:10,013 : INFO : PROGRESS: at 16.81% examples, 694015 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-31 14:36:11,015 : INFO : PROGRESS: at 18.76% examples, 691776 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:12,015 : INFO : PROGRESS: at 20.96% examples, 684146 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:13,026 : INFO : PROGRESS: at 22.55% examples, 681484 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:14,036 : INFO : PROGRESS: at 23.68% examples, 677197 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:15,049 : INFO : PROGRESS: at 24.63% examples, 663061 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:16,046 : INFO : PROGRESS: at 26.81% examples, 658968 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:17,059 : INFO : PROGRESS: at 28.94% examples, 659615 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:18,062 : INFO : PROGRESS: at 30.37% examples, 658944 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-31 14:36:19,066 : INFO : PROGRESS: at 31.87% examples, 658240 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-31 14:36:20,073 : INFO : PROGRESS: at 33.95% examples, 657666 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:21,092 : INFO : PROGRESS: at 35.93% examples, 658170 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-31 14:36:22,098 : INFO : PROGRESS: at 37.81% examples, 658518 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:23,105 : INFO : PROGRESS: at 39.82% examples, 656201 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:24,108 : INFO : PROGRESS: at 41.76% examples, 654496 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:25,105 : INFO : PROGRESS: at 42.82% examples, 648919 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-31 14:36:26,117 : INFO : PROGRESS: at 43.81% examples, 646438 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-31 14:36:27,115 : INFO : PROGRESS: at 45.40% examples, 643987 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-31 14:36:28,124 : INFO : PROGRESS: at 47.50% examples, 641628 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:29,141 : INFO : PROGRESS: at 49.27% examples, 640513 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:30,161 : INFO : PROGRESS: at 50.53% examples, 638244 words/s, in_qsize 8, out_qsize 1\n",
      "2017-03-31 14:36:31,167 : INFO : PROGRESS: at 51.77% examples, 634905 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:32,186 : INFO : PROGRESS: at 53.86% examples, 635069 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-31 14:36:33,184 : INFO : PROGRESS: at 55.75% examples, 635448 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:34,190 : INFO : PROGRESS: at 57.35% examples, 633820 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:35,191 : INFO : PROGRESS: at 59.21% examples, 633412 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:36,194 : INFO : PROGRESS: at 61.38% examples, 632540 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-31 14:36:37,204 : INFO : PROGRESS: at 62.74% examples, 632168 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-31 14:36:38,227 : INFO : PROGRESS: at 63.83% examples, 632457 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:39,250 : INFO : PROGRESS: at 65.53% examples, 631319 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:40,253 : INFO : PROGRESS: at 67.50% examples, 629356 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-31 14:36:41,253 : INFO : PROGRESS: at 69.17% examples, 628034 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:42,280 : INFO : PROGRESS: at 70.56% examples, 627969 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-31 14:36:43,315 : INFO : PROGRESS: at 72.05% examples, 627639 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-31 14:36:44,332 : INFO : PROGRESS: at 74.00% examples, 626567 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:45,362 : INFO : PROGRESS: at 75.77% examples, 625716 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:46,359 : INFO : PROGRESS: at 77.39% examples, 624903 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:47,380 : INFO : PROGRESS: at 79.21% examples, 624357 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:48,378 : INFO : PROGRESS: at 81.38% examples, 623867 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-31 14:36:49,390 : INFO : PROGRESS: at 82.70% examples, 623145 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:50,397 : INFO : PROGRESS: at 83.71% examples, 622526 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:51,404 : INFO : PROGRESS: at 85.28% examples, 622380 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-31 14:36:52,414 : INFO : PROGRESS: at 87.54% examples, 622603 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:53,427 : INFO : PROGRESS: at 89.35% examples, 622908 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-31 14:36:54,433 : INFO : PROGRESS: at 90.77% examples, 623360 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:55,457 : INFO : PROGRESS: at 92.24% examples, 622823 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-31 14:36:56,463 : INFO : PROGRESS: at 93.87% examples, 620163 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:57,476 : INFO : PROGRESS: at 95.71% examples, 620246 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-31 14:36:58,484 : INFO : PROGRESS: at 97.29% examples, 619427 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-31 14:36:59,503 : INFO : PROGRESS: at 98.97% examples, 618502 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-31 14:36:59,940 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-03-31 14:36:59,964 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-03-31 14:36:59,969 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-03-31 14:36:59,970 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-03-31 14:36:59,971 : INFO : training on 49054080 raw words (36479564 effective words) took 59.0s, 617999 effective words/s\n",
      "2017-03-31 14:36:59,980 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-03-31 14:37:00,135 : INFO : saving Word2Vec object under 300features_40minwords_6context, separately None\n",
      "2017-03-31 14:37:00,138 : INFO : not storing attribute syn0norm\n",
      "2017-03-31 14:37:00,138 : INFO : not storing attribute cum_table\n",
      "2017-03-31 14:37:00,694 : INFO : saved 300features_40minwords_6context\n"
     ]
    }
   ],
   "source": [
    "##import logging\n",
    "##logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 6           # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "from gensim.models import word2vec\n",
    "print \"Training model...\"\n",
    "model = word2vec.Word2Vec(data_splitted, workers=num_workers, size=num_features, min_count = min_word_count, \n",
    "                          window = context, sample = downsampling)\n",
    "\n",
    "model.init_sims(replace=True)\n",
    "model_name = \"300features_40minwords_6context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10897L, 300L)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(text, model, num_features):\n",
    "    # Function to average all of the word vectors in a given paragraph\n",
    "    \n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    words = text.split(\" \")\n",
    "    \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "\n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "\n",
    "    counter = 0.\n",
    "\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "\n",
    "    for review in reviews:\n",
    "        if counter%1000. == 0.:\n",
    "            print \"Review %d of %d\" % (counter, len(reviews))\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 47706\n",
      "Review 1000 of 47706\n",
      "Review 2000 of 47706\n",
      "Review 3000 of 47706\n",
      "Review 4000 of 47706\n",
      "Review 5000 of 47706\n",
      "Review 6000 of 47706\n",
      "Review 7000 of 47706\n",
      "Review 8000 of 47706\n",
      "Review 9000 of 47706\n",
      "Review 10000 of 47706\n",
      "Review 11000 of 47706\n",
      "Review 12000 of 47706\n",
      "Review 13000 of 47706\n",
      "Review 14000 of 47706\n",
      "Review 15000 of 47706\n",
      "Review 16000 of 47706\n",
      "Review 17000 of 47706\n",
      "Review 18000 of 47706\n",
      "Review 19000 of 47706\n",
      "Review 20000 of 47706\n",
      "Review 21000 of 47706\n",
      "Review 22000 of 47706\n",
      "Review 23000 of 47706\n",
      "Review 24000 of 47706\n",
      "Review 25000 of 47706\n",
      "Review 26000 of 47706\n",
      "Review 27000 of 47706\n",
      "Review 28000 of 47706\n",
      "Review 29000 of 47706\n",
      "Review 30000 of 47706\n",
      "Review 31000 of 47706\n",
      "Review 32000 of 47706\n",
      "Review 33000 of 47706\n",
      "Review 34000 of 47706\n",
      "Review 35000 of 47706\n",
      "Review 36000 of 47706\n",
      "Review 37000 of 47706\n",
      "Review 38000 of 47706\n",
      "Review 39000 of 47706\n",
      "Review 40000 of 47706\n",
      "Review 41000 of 47706\n",
      "Review 42000 of 47706\n",
      "Review 43000 of 47706\n",
      "Review 44000 of 47706\n",
      "Review 45000 of 47706\n",
      "Review 46000 of 47706\n",
      "Review 47000 of 47706\n"
     ]
    }
   ],
   "source": [
    "X = getAvgFeatureVecs(data.content, model, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47706L, 300L)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, data.sender.values, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35779L, 300L)\n",
      "(11927L, 300L)\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "notnoneix = ~np.isnan(X_train).any(axis=1)\n",
    "notnoneix_test = ~np.isnan(X_test).any(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35770L, 300L)\n",
      "(11923L, 300L)\n"
     ]
    }
   ],
   "source": [
    "print X_train[notnoneix].shape\n",
    "print X_test[notnoneix_test].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.892896083201\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(np.nan_to_num(X_train[notnoneix]), y_train[notnoneix])\n",
    "\n",
    "print metrics.accuracy_score(y_test[notnoneix_test], clf.predict(X_test[notnoneix_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.93      0.93      2216\n",
      "          1       0.90      0.95      0.92      2193\n",
      "          2       0.88      0.92      0.90      1326\n",
      "          3       0.89      0.86      0.87      1256\n",
      "          4       0.93      0.89      0.91      1087\n",
      "          5       0.90      0.84      0.87       956\n",
      "          6       0.85      0.78      0.82       779\n",
      "          7       0.73      0.79      0.76       721\n",
      "          8       0.87      0.91      0.89       679\n",
      "          9       0.97      0.88      0.92       714\n",
      "\n",
      "avg / total       0.89      0.89      0.89     11927\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, clf.predict(np.nan_to_num(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
