{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I use sigmoid function as activation for hidden layer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    x = 1 / (1 + np.exp(-x))\n",
    "    return x\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    return (x) * (1-x)\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First I implement the classic Elman RNN also knows as simple recurrent network based on this work http://psych.colorado.edu/~kimlab/Elman1990.pdf . It doesnt unfold through time (truncated\n",
    "backpropagation was used in the original Elmans work) so it's very simple. The context (hidden[t-1]) was simply regarded as\n",
    "an additional input (on the left side of the image):**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"rnn.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class SRN:\n",
    "    def __init__(self, vocab_size, hidden_size, learn_rate):\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.Wx = np.random.randn(hidden_size, vocab_size) * 0.01 \n",
    "        self.U = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.Wy = np.random.randn(vocab_size, hidden_size) * 0.01\n",
    "        self.bias_hidden = np.zeros((hidden_size, 1))\n",
    "        self.bias_y = np.zeros((vocab_size, 1))\n",
    "        \n",
    "        self.context = np.zeros((hidden_size,1))\n",
    "        self.learn_rate = learn_rate\n",
    "        \n",
    "    def reset_context(self):\n",
    "        self.context = np.zeros_like(self.context)\n",
    "        \n",
    "    def train(self, input, target):\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        x[char_to_num[input]] = 1\n",
    "        target = char_to_num[target]\n",
    "        \n",
    "        #forward\n",
    "        hidden = sigmoid(np.dot(self.Wx, x) + np.dot(self.U, self.context) + self.bias_hidden)\n",
    "        self.context = np.copy(hidden)\n",
    "        y = np.dot(self.Wy, hidden) + self.bias_y\n",
    "\n",
    "        probs = softmax(y)            \n",
    "        loss = -np.log(probs[target, 0])\n",
    "        \n",
    "        #backprop\n",
    "        dy = np.copy(probs)\n",
    "        dy[target] -= 1\n",
    "        dbias_y = dy\n",
    "        dWy = np.dot(dy, hidden.T)            \n",
    "\n",
    "        dhidden = sigmoid_grad(hidden) * (np.dot(self.Wy.T, dy)) \n",
    "        dbias_hidden = dhidden\n",
    "\n",
    "        dWx = np.dot(dhidden, x.T)\n",
    "        dU = np.dot(dhidden, self.context.T)\n",
    "        \n",
    "        #param update \n",
    "         \n",
    "        self.Wx += -self.learn_rate * dWx\n",
    "        self.Wy += -self.learn_rate * dWy\n",
    "        self.U += -self.learn_rate * dU\n",
    "        self.bias_hidden += -self.learn_rate * dbias_hidden\n",
    "        self.bias_y += - self.learn_rate * dbias_y\n",
    " \n",
    "        return loss\n",
    "    \n",
    "    def predict(self, amount, start_el):\n",
    "        predics = []\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        x[char_to_num[start_el]] = 1\n",
    "        \n",
    "        hidden = np.copy(self.context)\n",
    "\n",
    "        for t in range(amount):\n",
    "            hidden = sigmoid(np.dot(self.Wx, x) + np.dot(self.U, hidden) + self.bias_hidden)\n",
    "            y = np.dot(self.Wy, hidden) + self.bias_y\n",
    "\n",
    "            probs = softmax(y)\n",
    "            ix = np.random.choice(range(self.vocab_size), p= probs.ravel())\n",
    "            predics.append(ix)\n",
    "            #predics.append(np.argmax(probs))\n",
    "\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[ix] = 1\n",
    "\n",
    "        return ''.join(num_to_char[ix] for ix in predics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text size: 99993\n",
      "Epoch: 0\n",
      "Iteration:  10000 , average loss:  3.25361170453\n",
      "Sample:  touwh :senmn doeHl:e  Eedn  theEtag loReeeG dor:\n",
      "urnlbeo ,\n",
      "dist lm'HG du.fali ee n  mMeole,.\n",
      "lhth t \n",
      "  \n",
      "Iteration:  20000 , average loss:  3.03968336464\n",
      "Sample:   erI mihooTs er\n",
      ":Ce smithewriciy fireIrpoc os S\n",
      "Einwoyoe f s Ttkee  hIev\n",
      "d f  t fAbweG egosdindt ula\n",
      "  \n",
      "Iteration:  30000 , average loss:  2.99101845237\n",
      "Sample:  Am lfd  te.,UreAAre, ? e a?M:iralI arthou ferveeDn etse yle\n",
      "IonR\n",
      "he, thr bebie baenu 'sre Ith, waof\n",
      "\n",
      "  \n",
      "Iteration:  40000 , average loss:  2.94292884998\n",
      "Sample:  ,innd wststle tf oNupsaen kLLhemethoreamunoeyceelrea\n",
      "amalll ks er:\n",
      "d l s, o j, anre cns Ero wt O-Ihr\n",
      "  \n",
      "Iteration:  50000 , average loss:  2.92818305171\n",
      "Sample:  ,I onNSeeeeg,berehiha by h \n",
      "Un O sedanlou py drh:may  sYIuveye,t:Y y,irl ofil'dsdheN:nddikp,\n",
      "e somie\n",
      "  \n",
      "Iteration:  60000 , average loss:  2.90854702316\n",
      "Sample:  koUSchths tole aRA\n",
      "IAn, reiprgk d g ha san:\n",
      "saCPsso lsmeaninreow LotHecar oratmhQ\n",
      "e  shefwy locod lo\n",
      "  \n",
      "Iteration:  70000 , average loss:  2.89856641781\n",
      "Sample:  esaasity  nrto  I kFatoesngle\n",
      "Le l.daulme nked d measatbrt s d hl\n",
      "Ct  sor hd y elat lId ws ha w, see\n",
      "  \n",
      "Iteration:  80000 , average loss:  2.90111871177\n",
      "Sample:  hnd p hunouhofwidun,\n",
      "un by ealdd  monf  I\n",
      "Aciatomagvetht\n",
      "'ghileryivG YofontHethe Mlc, dd  aos hl o a\n",
      "  \n",
      "Iteration:  90000 , average loss:  2.89992082264\n",
      "Sample:  rI asng n afue epe ndtiofre,  t thi bnsurlecandCl vsssaat tn k aropeesi aar\n",
      "Roet  wssyo fou i fenth\n",
      "\n",
      "  \n",
      "Epoch: 1\n",
      "Iteration:  10000 , average loss:  2.87158141762\n",
      "Sample:  t:\n",
      "pee vehys hoirt.ed ft  tnoanAnbue seunm o w ak sreudkssoreuso e e e t.ed,\n",
      "hd:\n",
      "erraalNM dr\n",
      "s ou t\n",
      "\n",
      "  \n",
      "Iteration:  20000 , average loss:  2.8608025656\n",
      "Sample:  elay  lofr tin,Buets; s fatyethli fmritanbeonl' touthciir.\n",
      "opntfuusd yon ,\n",
      " bfo aheUS\n",
      "\n",
      "ARRTNA.,\n",
      " ?O \n",
      "  \n",
      "Iteration:  30000 , average loss:  2.85675795301\n",
      "Sample:  :\n",
      "HorCMeoaloerayun tm hetho:\n",
      "\n",
      "ai:\n",
      "pan ouy heo s erpae JO MUAarThthin t I\n",
      "W\n",
      "S OuSAeOINEOIS:DeAaNSiiAS\n",
      "  \n",
      "Iteration:  40000 , average loss:  2.84484227237\n",
      "Sample:  itbe\n",
      "Ct OLenlalot e rdetseesonmeny huld areaou hs ban, tmey,lls  gllhestr wanoy holowi\n",
      "Hfaknwi ehaoo\n",
      "  \n",
      "Iteration:  50000 , average loss:  2.845662215\n",
      "Sample:  enod e tn pllbet hellUn\n",
      "S AhahepoTRthou t g\n",
      "AMy II uFt,s t.nd\n",
      "\n",
      ",By ndd,s  hbu mAnabcot.ALju nob\n",
      "E ov\n",
      "  \n",
      "Iteration:  60000 , average loss:  2.84894566799\n",
      "Sample:  \n",
      "T meny veth tomse oNopkI t its archseasndl ins n d?le t glialosheG'he\n",
      "\n",
      "WBhas d hew Nfrsd mi tesn hi\n",
      "  \n",
      "Iteration:  70000 , average loss:  2.84354611687\n",
      "Sample:   n,\n",
      "Y gelwoliINr  ttsY :\n",
      "heitK\n",
      "wiiryail aurlytendOSei merhin e  gl  iutt. hYuo e  br hey s:meote 'sg\n",
      "  \n",
      "Iteration:  80000 , average loss:  2.85394787957\n",
      "Sample:  hu\n",
      " houe dsus fumdiouldoren may nulutimThea t-yss hNGy  mes o f'Dcowhby wt n ell \n",
      "Bitstt erBuhogo ts\n",
      "  \n",
      "Iteration:  90000 , average loss:  2.84822834308\n",
      "Sample:  omtmyn ha bdeikt auacosI hee huo omp p ai l ill n tov tce fflmyei ayoVLcen.s .\n",
      "inw shNofnst\n",
      "SHos ust\n",
      "  \n",
      "Epoch: 2\n",
      "Iteration:  10000 , average loss:  2.83071984814\n",
      "Sample:  mndONhawafak .\n",
      "ar l\n",
      "SI whveu en m alluecu.\n",
      "d\n",
      "\n",
      "C df's is\n",
      "Tda g\n",
      "\n",
      "tIpeayanleobndo'howi ddeh utti i s tw\n",
      "  \n",
      "Iteration:  20000 , average loss:  2.83552873445\n",
      "Sample:  ,St o\n",
      "\n",
      "eUoNNAOMEANUCNo udt esorcendsh fesleatin hlot  aOPIUou ftola, ilbaerreo.grnc nany\n",
      " m k tleou\n",
      "\n",
      "  \n",
      "Iteration:  30000 , average loss:  2.84031542554\n",
      "Sample:  KINOARLOd ANY:\n",
      "Cne,\n",
      "Mafa se'n whn atar\n",
      "Witf arfres.\n",
      "hiSaw s ha it lime:\n",
      "\n",
      "Mitinwa frtlat e.f Uwy ayth\n",
      "  \n",
      "Iteration:  40000 , average loss:  2.81893879351\n",
      "Sample:   mveldbe wchAne :\n",
      "odrehorvt  hs Sorv if ofEC cwiru Twoinrsedend,in a b sd, tul y aafuc? LEpa sat w I\n",
      "  \n",
      "Iteration:  50000 , average loss:  2.82277717195\n",
      "Sample:  \n",
      "w  breefu wihie , esan b hl edplofeee nog d\n",
      "llor gmywrby mteolenedtrnte sh f yel.  aosl Anme n 'al \n",
      "  \n",
      "Iteration:  60000 , average loss:  2.8293575331\n",
      "Sample:  er cst, wa m ys,n mtasares, bjr isonr ncstr the ne canre\n",
      "Bha iancamao iterdsu!srr KOI erwi cto\n",
      "A dha\n",
      "  \n",
      "Iteration:  70000 , average loss:  2.83072304689\n",
      "Sample:  auvewelrealal t lo  astfureesowhe\n",
      "Wacglinornota Irara yt \n",
      "Lano\n",
      "rendUShahiore onh ree ve cce arewem h\n",
      "  \n",
      "Iteration:  80000 , average loss:  2.83235059033\n",
      "Sample:   isllmecaf:st lhss  fh, fICr,inng thoel aut v, che heeftas.s an wy\n",
      "haanl buKoKewew toorARroy;al gHfr\n",
      "  \n",
      "Iteration:  90000 , average loss:  2.83108008252\n",
      "Sample:  ahiWhonw to a\n",
      "\n",
      "r: aNheaoe Cetiten,yoef\n",
      "Aieh. houfu dngveisrayoed Lnch,h vee;qu aav fwi n tno tndtoal\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "text = open('shake.txt', 'r').read()\n",
    "print \"Text size:\", len(text)\n",
    "chars = list(set(text))\n",
    "vocab_size = len(chars)\n",
    "char_to_num = {ch : i for i, ch in enumerate(chars)}\n",
    "num_to_char = {i : ch for i, ch in enumerate(chars)}\n",
    "\n",
    "model_simple = SRN(vocab_size, 128, 0.1)\n",
    "\n",
    "for epoch in range(3):\n",
    "    print \"Epoch:\", epoch \n",
    "    loss = .0\n",
    "    \n",
    "    model.reset_context()\n",
    "    for j in range(0, len(text) - 2):\n",
    "        loss += model_simple.train(text[j], text[j+1])\n",
    "        \n",
    "        if ((j % 10000 == 0) & (j > 0)):\n",
    "            print \"Iteration: \", j, \", average loss: \", loss/10000\n",
    "            print \"Sample: \", model_simple.predict(100, text[j])\n",
    "            print \"  \"\n",
    "            loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As we see it doesnt converge very well (I didn't try any advanced techniques for backpropagation). It learned to break words/lines some letters dependence but it is not able to build language model due to its simplicity. The original idea of Elman was to learn XOR sequences and simple letter sequences like these:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 0\n",
      "Iteration:  10000 , average loss:  0.825909765546\n",
      "Sample:  iiiaeciciaebuuuaeaebuuubuciiciiibuuuaebuuaeciiciiiiciiiciiiciiiaeaebuuciiciiiiibuuuaecibuuuaeciciici\n",
      "  \n",
      "Iteration:  20000 , average loss:  0.563045342452\n",
      "Sample:  aebuuciiiaeciaeaebuuaeaeciiiciiiiciiibuuuaeaeaeaeaeaeaeciiiaeaeaeaeaeaeaeaebuuaebuuaeaeaeaeaeaeciiii\n",
      "  \n",
      "Epoch: 1\n",
      "Iteration:  10000 , average loss:  0.514185060425\n",
      "Sample:  ibuuuaeciiibuuaebuuuaeciiiaeciiciiiaeciiciiiciiaeaebuuuaeciiibuuuaebuuciiiciiiiaebuuciiibuubuuuaecii\n",
      "  \n",
      "Iteration:  20000 , average loss:  0.529015320716\n",
      "Sample:  ciiiiiaeaeaeaeaeaeciiiciiiciiiaebuuaeaeaeaeaeaebuuciiiuaeaeaeciiiiciiiiaebuuciiiciiiciiiaeaeaeaeciia\n",
      "  \n",
      "Epoch: 2\n",
      "Iteration:  10000 , average loss:  0.581612858615\n",
      "Sample:  iaeciaeaeaeciiiciiiiciiiciiiiciiiiaeaebuuaeaebuuaeaeciiiaeciiiaebuubuciiaeaeciiiiibuuucaeaeaeciiiici\n",
      "  \n",
      "Iteration:  20000 , average loss:  0.562677598285\n",
      "Sample:  abuuaeciiciiiciiiiaeciibuuciaebuuciiaeciiiibuuciciiiaeaeaeaebuuaeaeaebuuaeciiibuuuaebuuaeciiiciiiaea\n",
      "  \n",
      "Epoch: 3\n",
      "Iteration:  10000 , average loss:  0.520375132363\n",
      "Sample:  iciiiiaebuuaeaeciiiciiiciiiiaebuuuaeaeciiiaeciiiiaebuuuaeaeciiiaeciiiiaeaeciiaeciiiiaeaeaeciiiaeaeci\n",
      "  \n",
      "Iteration:  20000 , average loss:  0.50184666313\n",
      "Sample:  ciaeaeaebuuciibuuciiciiiaeaeaeaebuuaeaeciiiaeciiiaebuubuaeaebuuciiiciiiaeaeaeaebuuciibuuaeaeaeciaeci\n",
      "  \n",
      "Epoch: 4\n",
      "Iteration:  10000 , average loss:  0.502592517372\n",
      "Sample:  aeciiiaeciiiciiaebuuaebuuaebuuaebuubuubuuciiaeciiiciiiaeciiiaeaeciiaeciiciiiaebuuaeciiciiaeaeciiaeae\n",
      "  \n",
      "Iteration:  20000 , average loss:  0.490857989628\n",
      "Sample:  buuuciiaebuuaeaebuuuaeaeciiaebuuaeaeaeaeciiibuuciibuuaeaebuuaeciciiiaeaeciiaeaeaeaebuuaeciiaeaeaecib\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "text_ = np.array([\"a\"] * 10000)\n",
    "text_[np.random.randint(0, 10000,3000)] = 'b'\n",
    "text_[np.random.randint(0, 10000,3000)] = 'c'\n",
    "text = ''.join(text_).replace('a', 'ae').replace('b', 'buu').replace('c','ciii')\n",
    "\n",
    "chars = list(set(text))\n",
    "vocab_size = len(chars)\n",
    "char_to_num = {ch : i for i, ch in enumerate(chars)}\n",
    "num_to_char = {i : ch for i, ch in enumerate(chars)}\n",
    "\n",
    "model_simple = SRN(vocab_size, 32, 0.1)\n",
    "\n",
    "for epoch in range(5):\n",
    "    print \"Epoch:\", epoch \n",
    "    loss = .0\n",
    "    \n",
    "    model_simple.reset_context()\n",
    "    for j in range(0, len(text) - 2):\n",
    "        loss += model_simple.train(text[j], text[j+1])\n",
    "        \n",
    "        if ((j % 10000 == 0) & (j > 0)):\n",
    "            print \"Iteration: \", j, \", average loss: \", loss/10000\n",
    "            print \"Sample: \", model_simple.predict(100, text[j])\n",
    "            print \"  \"\n",
    "            loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We see it works much better for simple sequences.\n",
    "Now let's go back to Shakespeare and let's try to advance our model with backpropagation through time (https://en.wikipedia.org/wiki/Backpropagation_through_time). The idea is to backpropagate errors through layers for fixed size batches. We go forward through batch and save all weights. Then we \"unfold\" the layers backpropagating errors \"through time\". The import derivative we add is dhidden_next that is the error from t+1-th step to t-th step.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNN(SRN):   \n",
    "    def __init__(self, vocab_size, hidden_size, learn_rate):\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.Wx = np.random.randn(hidden_size, vocab_size) * 0.01 \n",
    "        self.U = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.Wy = np.random.randn(vocab_size, hidden_size) * 0.01\n",
    "        self.bias_hidden = np.zeros((hidden_size, 1))\n",
    "        self.bias_y = np.zeros((vocab_size, 1))\n",
    "        \n",
    "        self.context = np.zeros((hidden_size,1))\n",
    "        self.learn_rate = learn_rate\n",
    "        \n",
    "        #we save squared weights for gradients for adagrad \n",
    "        self.adaU = np.zeros((hidden_size, hidden_size))\n",
    "        self.adaWx = np.zeros((hidden_size, vocab_size))\n",
    "        self.adaWy = np.zeros((vocab_size, hidden_size))\n",
    "        self.adabias_hidden = np.zeros((hidden_size, 1))\n",
    "        self.adabias_y = np.zeros((vocab_size, 1))\n",
    "        \n",
    "    def train_batch(self, inputs, targets):\n",
    "        batch_loss = 0.0\n",
    "        #history\n",
    "        x = {}\n",
    "        y = {}\n",
    "        hidden = {}\n",
    "        hidden[-1] = np.copy(self.context)\n",
    "        probs = {}\n",
    "        \n",
    "        #init incremental gradients\n",
    "        dWx = np.zeros_like(self.Wx)\n",
    "        dU = np.zeros_like(self.U)\n",
    "        dWy = np.zeros_like(self.Wy)\n",
    "        dbias_hidden = np.zeros_like(self.bias_hidden)\n",
    "        dbias_y = np.zeros_like(self.bias_y)\n",
    "        dhidden_next = np.zeros_like(self.context)\n",
    "        \n",
    "        #forward\n",
    "        for t in xrange(len(inputs)):\n",
    "            x[t] = np.zeros((self.vocab_size, 1))\n",
    "            x[t][inputs[t]] = 1\n",
    "            \n",
    "            hidden[t] = sigmoid(np.dot(self.Wx, x[t]) + np.dot(self.U, hidden[t-1]) + self.bias_hidden)\n",
    "            y[t] = np.dot(self.Wy, hidden[t]) + self.bias_y\n",
    "            \n",
    "            probs[t] = softmax(y[t])\n",
    "            batch_loss +=  -np.log(probs[t][targets[t], 0])\n",
    "            \n",
    "        #backprop   \n",
    "        for t in reversed(xrange(len(inputs))):\n",
    "            dy = np.copy(probs[t])\n",
    "            dy[targets[t]] -= 1\n",
    "            \n",
    "            dbias_y += dy       \n",
    "            dWy += np.dot(dy, hidden[t].T)\n",
    "            \n",
    "            dhidden = sigmoid_grad(hidden[t]) * ((np.dot(self.Wy.T, dy)) + dhidden_next)\n",
    "            \n",
    "            dbias_hidden += dhidden\n",
    "            dWx += np.dot(dhidden, x[t].T)\n",
    "            dU += np.dot(dhidden, hidden[t-1].T)\n",
    "            \n",
    "            dhidden_next = np.dot(self.U.T, dhidden)\n",
    "            \n",
    "        #param update \n",
    "        for dparam in [dWx, dU, dWy, dbias_hidden, dbias_y]:\n",
    "            np.clip(dparam, -1, 1, out=dparam)\n",
    "            \n",
    "        #update RNN parameters according to Adagrad\n",
    "        for param, dparam, adaparam in zip([self.U, self.Wx, self.Wy, self.bias_hidden, self.bias_y], \\\n",
    "                                [dU, dWx, dWy, dbias_hidden, dbias_y], \\\n",
    "                                [self.adaW_hh, self.adaW_xh, self.adaW_hy, self.adab_h, self.adab_y]):\n",
    "            adaparam += dparam * dparam\n",
    "            param += -self.learn_rate * dparam/np.sqrt(adaparam + 1e-8) \n",
    "            \n",
    "        #self.Wx += -self.learn_rate * dWx\n",
    "        #self.Wy += -self.learn_rate * dWy\n",
    "        #self.U += -self.learn_rate * dU\n",
    "        #self.bias_hidden += -self.learn_rate * dbias_hidden\n",
    "        #self.bias_y += - self.learn_rate * dbias_y\n",
    "        \n",
    "        self.context = hidden[len(x)-1]\n",
    "        return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are two main problems here: vanishing and exploding grad descent problems (http://proceedings.mlr.press/v28/pascanu13.pdf). Exploding problem could be fixed clipping gradients to const value (-1 and 1 in my case). Also I didnt successed updating gradients as in the first implementation (It didn't converge at all!) so I updated that part with Adagrad (https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad) which use some kind of sparsity param update**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The batch_size is also the amount of layers we backpropagate to**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Iteration:  1000 , batch loss:  42.463785534\n",
      "Sample:  aur gat, ghiw att fetse TutNNisr the derint bee fuire toeunseot euw'de\n",
      "youd the kautiy ny nergWse of\n",
      "  \n",
      "Iteration:  2000 , batch loss:  36.1611592987\n",
      "Sample:  .\n",
      "\n",
      "GCR:CLI by'll derts al foe yoip ar you; cour on prer and poos.\n",
      "\n",
      "Mo mafrom dere faud spue hir bede\n",
      "  \n",
      "Iteration:  3000 , batch loss:  34.146330537\n",
      "Sample:   heganer ofei;\n",
      " othed av sore, prad sile thus rithrt, s 'ftes loun'ded, coreang bes, anve anjt a I a\n",
      "  \n",
      "Iteration:  4000 , batch loss:  33.9993924337\n",
      "Sample:  tus, on hill mattre doss,,\n",
      "Hir, Heest wiald\n",
      "Shensngwha and riwe have , lomes wors lut.\n",
      " br ave bet a\n",
      "  \n",
      "Iteration:  5000 , batch loss:  32.9134378005\n",
      "Sample:  er ind arf derse ofs whis:\n",
      "Bees, in alu dears: ofr uwweng riderr ind wer wills\n",
      "\n",
      "Brifem\n",
      "Jor hacr ame,\n",
      "  \n",
      "Iteration:  6000 , batch loss:  32.3726121883\n",
      "Sample:  f a my to shee you whas hath to he gruth\n",
      "I grus workzen, wher is prast undall, math weod and, mots.\n",
      "\n",
      "  \n",
      "Epoch: 1\n",
      "Iteration:  1000 , batch loss:  31.6479187428\n",
      "Sample:  d I tpee lith the seast;\n",
      "LeckeW he that of nee,\n",
      "Coulent jowancests, my thou gromes tit ames commalt \n",
      "  \n",
      "Iteration:  2000 , batch loss:  31.4430484905\n",
      "Sample:  art,\n",
      "And ag\n",
      "'nd be presaezpelins wive the.\n",
      "\n",
      "DHSOSEH:\n",
      "She theish Thf en, your you cerdestrither buse \n",
      "  \n",
      "Iteration:  3000 , batch loss:  30.7857752968\n",
      "Sample:  e doichm timeilt\n",
      "Thach med I shook,\n",
      "I 'lede;\n",
      "it hes aglsty ams:\n",
      "Wheal of nom pigfuy, Goy, coly clas \n",
      "  \n",
      "Iteration:  4000 , batch loss:  31.3138463677\n",
      "Sample:  ing If ay in hisssod tut'al wis, Lutsse the our all disiee thom alldfone and your fore\n",
      "I pringes ceo\n",
      "  \n",
      "Iteration:  5000 , batch loss:  30.7481798334\n",
      "Sample:  enon it hes qyol caex dadlimd nan;\n",
      "No wour lonc!\n",
      "Bue thee land dare thous ould to she Thould in doye\n",
      "  \n",
      "Iteration:  6000 , batch loss:  30.4628184879\n",
      "Sample:  s heven, I woutl bemant bleach mistenge must and heoutoucd-'d us tam vith;,\n",
      "Thy ast this I yeur mome\n",
      "  \n",
      "Epoch: 2\n",
      "Iteration:  1000 , batch loss:  30.1336501049\n",
      "Sample:  d thinal pailedt if strole,\n",
      "And hipe my wrard hear ucsenty, To ever peay heriss your cunEy have thiy\n",
      "  \n",
      "Iteration:  2000 , batch loss:  30.0991211954\n",
      "Sample:  sign, wimn wim, hes Sane, as pae ope yhu youl his hove gist fere.\n",
      "\n",
      "OUCTULES:\n",
      "Reware beetuy in goftin\n",
      "  \n",
      "Iteration:  3000 , batch loss:  29.5461327698\n",
      "Sample:  .\n",
      "\n",
      "DANDOR:\n",
      "EDaind, Wolle\n",
      "Peem your in and ear, and neverted\n",
      "I bures\n",
      "Thith, I kink noct go isseat, hi\n",
      "  \n",
      "Iteration:  4000 , batch loss:  30.1719307257\n",
      "Sample:  turt fanes-fulls seaveris's! anr eve, I prnet to me to to spraft I pilmy, I ho and Thot clast deptor\n",
      "  \n",
      "Iteration:  5000 , batch loss:  29.7181441504\n",
      "Sample:  ein demate the wrall appay'd at ne seg'lly will thai; he, Butsend her have ow well\n",
      "Ky me 'tull memer\n",
      "  \n",
      "Iteration:  6000 , batch loss:  29.4496126993\n",
      "Sample:  t mur ker.\n",
      "\n",
      "CARtARA:\n",
      "\n",
      "all is to-, whar: beer loop: Iow my fordacesy''d\n",
      "To thes you amire herborss, s\n",
      "  \n",
      "Epoch: 3\n",
      "Iteration:  1000 , batch loss:  29.236371926\n",
      "Sample:  cher.\n",
      "\n",
      "CALETHUS:\n",
      "O peid evech the neteat fage be, get ellings and, lighels;\n",
      "Ceciwn;\n",
      "of thit it hangu\n",
      "  \n",
      "Iteration:  2000 , batch loss:  29.2340825337\n",
      "Sample:  O, see, kip frout.\n",
      "\n",
      "ORI:\n",
      "Whep now to there: whith et:\n",
      "A liven\n",
      "Sery welldid,\n",
      "Whincue coivon as and ou\n",
      "  \n",
      "Iteration:  3000 , batch loss:  28.7575811842\n",
      "Sample:  ;\n",
      "Anforinkers om.\n",
      "At vellioth and ain,\n",
      "Whee hish heare wigh eves?.\n",
      "\n",
      "COGINAL:\n",
      "Selought Gipe to frh ve\n",
      "  \n",
      "Iteration:  4000 , batch loss:  29.4150815427\n",
      "Sample:  senter, Cantwerou\n",
      "That som he, and rise mated brum thuor loys at lesy deast prether Her to butsen, m\n",
      "  \n",
      "Iteration:  5000 , batch loss:  29.0080717324\n",
      "Sample:  rich's with trise be bellore.\n",
      "\n",
      "SAKE al:\n",
      "And the have at ous to thy everep yover's ruser groud Prepe.\n",
      "  \n",
      "Iteration:  6000 , batch loss:  28.7445370878\n",
      "Sample:  nes:\n",
      "and withisiryian at parlert berow wathel the wairit.\n",
      "\n",
      "ANTEAR:\n",
      "What a was meker peack all serean\n",
      "  \n",
      "Epoch: 4\n",
      "Iteration:  1000 , batch loss:  28.5897736205\n",
      "Sample:  say leessen ut my hemmare to at should pore there\n",
      "And mear it challed;\n",
      "Ferteet and seribent if doy, \n",
      "  \n",
      "Iteration:  2000 , batch loss:  28.6035476122\n",
      "Sample:  Ole naby wall kin erean, you so not to gramine prey thes stand of wirged is kenounm of to mo cotpo c\n",
      "  \n",
      "Iteration:  3000 , batch loss:  28.190773037\n",
      "Sample:   he han areant,\n",
      "To reats you lit the herowe war: the protore nat semingse, .\n",
      "Whue of fannd nom.\n",
      "\n",
      "KIR\n",
      "  \n",
      "Iteration:  4000 , batch loss:  28.8573012894\n",
      "Sample:  iped arleBrtulest on Crust and the pay stove bo ler: hore her salisd you brately a ar to breathoun s\n",
      "  \n",
      "Iteration:  5000 , batch loss:  28.4711488332\n",
      "Sample:  itise a wazh atuse ma?\n",
      " ave anfurst, peemanes by brapher yem to lupsly 'Tid what he thou frowse mour\n",
      "  \n",
      "Iteration:  6000 , batch loss:  28.216119277\n",
      "Sample:  n nase to he tond, you rmire.\n",
      "\n",
      "KING CFOTCIUG:\n",
      "That inknew of then knor now and so\n",
      "Brasce for got Her\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "text = open('shake.txt', 'r').read()\n",
    "chars = list(set(text))\n",
    "vocab_size = len(chars)\n",
    "char_to_num = {ch : i for i, ch in enumerate(chars)}\n",
    "num_to_char = {i : ch for i, ch in enumerate(chars)}\n",
    "\n",
    "batch_size = 15\n",
    "model = RNN(vocab_size, 128, 0.1)\n",
    "\n",
    "#I train only for 5 epochs but it still converges so we can increase this one!\n",
    "for epoch in range(5):\n",
    "    print \"Epoch:\", epoch \n",
    "    loss = .0\n",
    "    model.reset_context()\n",
    "    \n",
    "    for j in range(0, len(text)/batch_size):\n",
    "        X = [char_to_num[ch] for ch in text[j * batch_size:(j+1) * batch_size]]\n",
    "        Y = [char_to_num[ch] for ch in text[j * batch_size+1:(j+1) * batch_size + 1]]\n",
    "        loss += model.train_batch(X, Y)\n",
    "        \n",
    "        if ((j % 1000 == 0) & (j > 0)):\n",
    "            print \"Iteration: \", j, \", batch loss: \", loss/1000\n",
    "            print \"Sample: \", model.predict(100, text[j])\n",
    "            print \"  \"\n",
    "            loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** We see model that backpropagates in time is able to learn syntactic rules much better than the simple one. It builds language model for Shakespeare poems and plays:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORE:\n",
      "A distus acking':\n",
      "Oud bastan.\n",
      "\n",
      "ArFOTRUS:\n",
      "Do hith moy, sprefing,\n",
      "Ed: toud, could,\n",
      "'veald eest we it have live howste.\n",
      "\n",
      "Werbust Moht thy fores a dosterunt\n",
      "eare I wa blames ou\n",
      "\n",
      "Wurs, I'll spferomver'd soon a would not!\n",
      "Well knemest mane in share gad; ho? a grod shalk a blows,:\n",
      "Of the stort Tith me, by your spight if the winger's.\n",
      "The sones.\n",
      "\n",
      "MAYs OLAUS:\n",
      "My calloes in mive palily hain the yearvels: poust as youl, ands heave the gave hart\n",
      "To duered:\n",
      "Lo kniey hay on extome\n",
      "Whe fove thy, preaver biteer have have theess. But of re. Mognend, I'll beip netlown; and have pyoed, we saver your the will not leok\n",
      "And botting; bet way seigh abe uo stear do not wids glakn dat indere, And Arin widr toep in in hh morl to moth mother my bedithing\n",
      "O tidrougd dowie; with toke.\n",
      "\n",
      "FLosth LIAs:\n",
      "Nell stofes gained fith inguef:\n",
      "Muve he labe peach and the grotnes my the gave houl,\n",
      "Ami\n",
      "the I Sagwanins fight I In quegon theoud be, a gady,\n",
      "The with lance whpeed bropsuce brat I and Seembung yet the cuw\n",
      "Tome\n",
      "Wew, \n"
     ]
    }
   ],
   "source": [
    "print model.predict(1000, text[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
